{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the Python Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the necessary Python packages found in requirements.txt and import the necessary Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import json\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import uuid\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from pyspark.sql import SparkSession, Window, functions as F\n",
    "from pyspark.sql.types import (\n",
    "    DateType,\n",
    "    DoubleType,\n",
    "    IntegerType,\n",
    "    LongType,\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    TimestampType,\n",
    ")\n",
    "from numpy import newaxis\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.utils import get_stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the Python notebook environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A sentence tokenizer to split text into sentences\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "# Initialize a Spark session\n",
    "# spark = SparkSession.builder.appName(\"Stock Forecasting\").getOrCreate()\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Stock Forecasting\")\n",
    "    .config(\"spark.executor.memory\", \"4g\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.default.parallelism\", \"8\")\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define global constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stock symbol\n",
    "STOCK_SYMBOL_UPPER = \"AA\"  # AA, AAPL, AMZN, MSFT, TSLA, JPM, LLY\n",
    "STOCK_SYMBOL_LOWER = \"aa\"  # aa, aapl, amzn, msft, tsla, jpm, lly\n",
    "\n",
    "# File names\n",
    "FILE_NAME_EXTERNAL_NEWS = f\"external_{STOCK_SYMBOL_LOWER}.csv\"\n",
    "FILE_NAME_NASDAQ_NEWS = f\"nasdaq_{STOCK_SYMBOL_LOWER}.csv\"\n",
    "FILE_NAME_NEWS = f\"news_{STOCK_SYMBOL_LOWER}.csv\"\n",
    "FILE_NAME_PRICE = f\"price_{STOCK_SYMBOL_LOWER}.csv\"\n",
    "FILE_NAME_COMBINED = f\"{STOCK_SYMBOL_LOWER}.csv\"\n",
    "\n",
    "# File paths\n",
    "FILE_PATH_ALL_EXTERNAL_NEWS = \"stock_news/external/all_external.csv\"\n",
    "FILE_PATH_EXTERNAL_NEWS = f\"stock_news/external/external_{STOCK_SYMBOL_LOWER}.csv\"\n",
    "FILE_PATH_ALL_NASDAQ_NEWS = \"stock_news/nasdaq/nasdaq_external_data.csv\"\n",
    "FILE_PATH_NASDAQ_NEWS = f\"stock_news/nasdaq/nasdaq_{STOCK_SYMBOL_LOWER}.csv\"\n",
    "FILE_PATH_MERGED_NEWS = f\"stock_news/merged/{FILE_NAME_NEWS}\"\n",
    "FILE_PATH_SUMMARIZED_NEWS = f\"stock_news/summarized/{FILE_NAME_NEWS}\"\n",
    "FILE_PATH_SCORED_NEWS = f\"stock_news/scored/{FILE_NAME_NEWS}\"\n",
    "FILE_PATH_DECAYED_NEWS = f\"stock_news/decayed/{FILE_NAME_NEWS}\"\n",
    "FILE_PATH_SENTIMENT_SCORES = f\"stock_news/scored/sentiments_{STOCK_SYMBOL_LOWER}.csv\"\n",
    "FILE_PATH_FULL_HISTORY_PRICE = f\"stock_price/full_history/{STOCK_SYMBOL_UPPER}.csv\"\n",
    "FILE_PATH_PREPROCESSED_PRICE = f\"stock_price/preprocessed/{FILE_NAME_PRICE}\"\n",
    "FILE_PATH_COMBINED = f\"stock_combined/{FILE_NAME_COMBINED}\"\n",
    "\n",
    "# Used by the Tokenizer\n",
    "LANGUAGE = \"english\"  # Set the language\n",
    "\n",
    "# Used by the LSA Summarizer\n",
    "SENTENCES_COUNT = 3  # Set the max number of sentences in a summary\n",
    "\n",
    "# Used by the LLM\n",
    "# MODEL = \"llama3.2:1b-instruct-fp16\"\n",
    "MODEL = \"llama3.2:3b-instruct-fp16\"  # Set the large-language model\n",
    "BATCH_SIZE = 5  # Set the max number of sentences in a batch to be fed to the LLM\n",
    "TEMPERATURE = 0.0  # Set the temperature to 0.0 for deterministic output\n",
    "MAX_OUTPUT_TOKENS = 14  # Set the maximum number of output tokens\n",
    "\n",
    "# Used by LLM for sentiment scoring\n",
    "MIN_VALUE = 1  # The minimum value of the sentiment score\n",
    "BASE_VALUE = 3  # The midpoint between 1 and 5 of the sentiment score\n",
    "MAX_VALUE = 5  # The maximum value of the sentiment score\n",
    "\n",
    "# Used by exponential decay algorithm\n",
    "DECAY_RATE = 0.5  # Determines how quickly the sentiment decays over time\n",
    "\n",
    "# Define the schema\n",
    "EXTERNAL_NEWS_SCHEMA = StructType(\n",
    "    [\n",
    "        StructField(\"Date\", TimestampType(), True),\n",
    "        StructField(\"Article_title\", StringType(), True),\n",
    "        StructField(\"Stock_symbol\", StringType(), True),\n",
    "        StructField(\"Url\", StringType(), True),\n",
    "        StructField(\"Publisher\", StringType(), True),\n",
    "        StructField(\"Author\", StringType(), True),\n",
    "        StructField(\"Article\", StringType(), True),\n",
    "        StructField(\"Lsa_summary\", StringType(), True),\n",
    "        StructField(\"Luhn_summary\", StringType(), True),\n",
    "        StructField(\"Textrank_summary\", StringType(), True),\n",
    "        StructField(\"Lexrank_summary\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "NASDAQ_NEWS_SCHEMA = StructType(\n",
    "    [\n",
    "        StructField(\"Unnamed: 0\", StringType(), True),\n",
    "        StructField(\"Date\", TimestampType(), True),\n",
    "        StructField(\"Article_title\", StringType(), True),\n",
    "        StructField(\"Stock_symbol\", StringType(), True),\n",
    "        StructField(\"Url\", StringType(), True),\n",
    "        StructField(\"Publisher\", StringType(), True),\n",
    "        StructField(\"Author\", StringType(), True),\n",
    "        StructField(\"Article\", StringType(), True),\n",
    "        StructField(\"Lsa_summary\", StringType(), True),\n",
    "        StructField(\"Luhn_summary\", StringType(), True),\n",
    "        StructField(\"Textrank_summary\", StringType(), True),\n",
    "        StructField(\"Lexrank_summary\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "MERGED_NEWS_SCHEMA = StructType(\n",
    "    [\n",
    "        StructField(\"Date\", TimestampType(), True),\n",
    "        StructField(\"Article_title\", StringType(), True),\n",
    "        StructField(\"Stock_symbol\", StringType(), True),\n",
    "        StructField(\"Url\", StringType(), True),\n",
    "        StructField(\"Publisher\", StringType(), True),\n",
    "        StructField(\"Author\", StringType(), True),\n",
    "        StructField(\"Article\", StringType(), True),\n",
    "        StructField(\"Lsa_summary\", StringType(), True),\n",
    "        StructField(\"Summarized\", IntegerType(), True),\n",
    "        StructField(\"Sentiment_score\", IntegerType(), True),\n",
    "        StructField(\"UUID\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "SUMMARIZED_NEWS_SCHEMA = StructType(\n",
    "    [\n",
    "        StructField(\"Date\", TimestampType(), True),\n",
    "        StructField(\"Article_title\", StringType(), True),\n",
    "        StructField(\"Stock_symbol\", StringType(), True),\n",
    "        StructField(\"Url\", StringType(), True),\n",
    "        StructField(\"Publisher\", StringType(), True),\n",
    "        StructField(\"Author\", StringType(), True),\n",
    "        StructField(\"Article\", StringType(), True),\n",
    "        StructField(\"Lsa_summary\", StringType(), True),\n",
    "        StructField(\"Summarized\", IntegerType(), True),\n",
    "        StructField(\"Sentiment_score\", IntegerType(), True),\n",
    "        StructField(\"UUID\", StringType(), True),\n",
    "        StructField(\"Text\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "SCORED_NEWS_SCHEMA = StructType(\n",
    "    [\n",
    "        StructField(\"UUID\", StringType(), True),\n",
    "        StructField(\"Date\", TimestampType(), True),\n",
    "        StructField(\"Sentiment_score\", IntegerType(), True),\n",
    "        StructField(\"Lsa_summary\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "DECAYED_SENTIMENT_SCHEMA = StructType(\n",
    "    [\n",
    "        StructField(\"Date\", DateType(), True),\n",
    "        StructField(\"Sentiment_avg\", DoubleType(), True),\n",
    "        StructField(\"Last_valid_sentiment\", DoubleType(), True),\n",
    "        StructField(\"Last_valid_date\", DateType(), True),\n",
    "        StructField(\"Days_since_last_valid\", IntegerType(), True),\n",
    "        StructField(\"Decayed_sentiment\", DoubleType(), True),\n",
    "    ]\n",
    ")\n",
    "PREPROCESSED_PRICE_SCHEMA = StructType(\n",
    "    [\n",
    "        StructField(\"Date\", DateType(), True),\n",
    "        StructField(\"Open\", DoubleType(), True),\n",
    "        StructField(\"High\", DoubleType(), True),\n",
    "        StructField(\"Low\", DoubleType(), True),\n",
    "        StructField(\"Close\", DoubleType(), True),\n",
    "        StructField(\"Adj_close\", DoubleType(), True),\n",
    "        StructField(\"Volume\", LongType(), True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the Price Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ingest the price dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read price data from CSV files to Spark dataframes\n",
    "df_price = spark.read.csv(FILE_PATH_FULL_HISTORY_PRICE, header=True, inferSchema=True)\n",
    "\n",
    "# Verify\n",
    "print(f\"Row count for df_price: {df_price.count()}\")\n",
    "df_price.printSchema()\n",
    "df_price.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the price dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the headers\n",
    "df_price_preprocessed = (\n",
    "    df_price.withColumnRenamed(\"date\", \"Date\")\n",
    "    .withColumnRenamed(\"open\", \"Open\")\n",
    "    .withColumnRenamed(\"high\", \"High\")\n",
    "    .withColumnRenamed(\"low\", \"Low\")\n",
    "    .withColumnRenamed(\"close\", \"Close\")\n",
    "    .withColumnRenamed(\"adj close\", \"Adj_close\")\n",
    "    .withColumnRenamed(\"volume\", \"Volume\")\n",
    ")\n",
    "\n",
    "# Reorder the columns\n",
    "df_price_preprocessed = df_price_preprocessed.select(\n",
    "    \"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Adj_close\", \"Volume\"\n",
    ")\n",
    "\n",
    "\n",
    "# Define a UDF to parse dates\n",
    "def parse_date(date_string):\n",
    "    for format in (\"%m/%d/%y\", \"%Y-%m-%d\"):\n",
    "        try:\n",
    "            return dt.datetime.strptime(date_string, format).date()\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return None  # Return None if no format matches\n",
    "\n",
    "\n",
    "# Register the UDF\n",
    "parse_date_udf = F.udf(parse_date, DateType())\n",
    "\n",
    "# Use the UDF to standardize the date format\n",
    "df_price_preprocessed = (\n",
    "    df_price_preprocessed.withColumn(\"Date\", parse_date_udf(F.col(\"Date\")))\n",
    "    .filter(F.col(\"Date\").isNotNull())\n",
    "    .orderBy(\"Date\")\n",
    ")\n",
    "\n",
    "# Round the DoubleType values to eight decimal places\n",
    "# Cast Date to DateType and Volume to LongType\n",
    "df_price_preprocessed = (\n",
    "    df_price_preprocessed.withColumn(\"Date\", F.col(\"Date\").cast(DateType()))\n",
    "    .withColumn(\"Open\", F.round(\"Open\", 8))\n",
    "    .withColumn(\"High\", F.round(\"High\", 8))\n",
    "    .withColumn(\"Low\", F.round(\"Low\", 8))\n",
    "    .withColumn(\"Close\", F.round(\"Close\", 8))\n",
    "    .withColumn(\"Adj_close\", F.round(\"Adj_close\", 8))\n",
    "    .withColumn(\"Volume\", F.col(\"Volume\").cast(LongType()))\n",
    ")\n",
    "\n",
    "# Verify\n",
    "print(f\"Row count for df_price: {df_price_preprocessed.count()}\")\n",
    "df_price_preprocessed.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save a copy of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify directories\n",
    "FOLDER_PRICE = \"stock_price/preprocessed\"\n",
    "TEMP_FOLDER_PRICE = f\"stock_price/preprocessed/price_{STOCK_SYMBOL_LOWER}\"\n",
    "\n",
    "# Write to a single CSV file\n",
    "df_price_preprocessed.coalesce(1).write.csv(\n",
    "    TEMP_FOLDER_PRICE, sep=\",\", header=True, mode=\"overwrite\"\n",
    ")\n",
    "\n",
    "# Move the part file to the desired filename\n",
    "for filename in os.listdir(TEMP_FOLDER_PRICE):\n",
    "    if filename.startswith(\"part-\"):\n",
    "        shutil.move(\n",
    "            os.path.join(TEMP_FOLDER_PRICE, filename),\n",
    "            os.path.join(FOLDER_PRICE, FILE_NAME_PRICE),\n",
    "        )\n",
    "\n",
    "# Remove the temporary directory\n",
    "shutil.rmtree(TEMP_FOLDER_PRICE)\n",
    "\n",
    "local_variables = [\n",
    "    \"df_price\",\n",
    "    \"df_price_preprocessed\",\n",
    "    \"parse_date_udf\",\n",
    "]\n",
    "\n",
    "# Delete local variables if they exist\n",
    "for var in local_variables:\n",
    "    if var in locals():  # Check if the variable exists in the local scope\n",
    "        del locals()[var]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract News Dataset A of a Stock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the all-external news CSV dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file to Spark dataframe\n",
    "df_all_news_external = spark.read.csv(\n",
    "    FILE_PATH_ALL_EXTERNAL_NEWS, header=True, schema=EXTERNAL_NEWS_SCHEMA\n",
    ")\n",
    "\n",
    "# Verify\n",
    "print(f\"Row count for df_all_news_external: {df_all_news_external.count()}\")\n",
    "df_all_news_external.printSchema()\n",
    "df_all_news_external.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract only the stock of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only the stock symbol of interest from the external news dataset\n",
    "df_news_external_filtered = df_all_news_external.filter(\n",
    "    F.col(\"Stock_symbol\") == STOCK_SYMBOL_UPPER\n",
    ").orderBy(\"Date\")\n",
    "\n",
    "# Verify\n",
    "print(f\"Row count for df_news_external_filtered: {df_news_external_filtered.count()}\")\n",
    "df_news_external_filtered.show(5, truncate=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save a copy of the external news dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify directories\n",
    "FOLDER_EXTERNAL = \"stock_news/external\"\n",
    "TEMP_FOLDER_EXTERNAL = f\"stock_news/external/external_{STOCK_SYMBOL_LOWER}\"\n",
    "\n",
    "# Write to a single CSV file\n",
    "df_news_external_filtered.coalesce(1).write.csv(\n",
    "    TEMP_FOLDER_EXTERNAL, sep=\",\", header=True, mode=\"overwrite\"\n",
    ")\n",
    "\n",
    "# Move the part file to the desired filename\n",
    "for filename in os.listdir(TEMP_FOLDER_EXTERNAL):\n",
    "    if filename.startswith(\"part-\"):\n",
    "        shutil.move(\n",
    "            os.path.join(TEMP_FOLDER_EXTERNAL, filename),\n",
    "            os.path.join(FOLDER_EXTERNAL, FILE_NAME_EXTERNAL_NEWS),\n",
    "        )\n",
    "\n",
    "# Remove the temporary directory\n",
    "shutil.rmtree(TEMP_FOLDER_EXTERNAL)\n",
    "\n",
    "local_variables = [\n",
    "    \"df_all_news_external\",\n",
    "    \"df_news_external_filtered\",\n",
    "]\n",
    "\n",
    "# Delete local variables if they exist\n",
    "for var in local_variables:\n",
    "    if var in locals():  # Check if the variable exists in the local scope\n",
    "        del locals()[var]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract News Dataset B of a Stock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reads in the all-Nasdaq news CSV dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file to Spark dataframe\n",
    "df_all_news_nasdaq = spark.read.csv(\n",
    "    FILE_PATH_ALL_NASDAQ_NEWS, header=True, schema=NASDAQ_NEWS_SCHEMA\n",
    ")\n",
    "\n",
    "# Verify\n",
    "print(f\"Row count for df_all_news_nasdaq: {df_all_news_nasdaq.count()}\")\n",
    "df_all_news_nasdaq.printSchema()\n",
    "df_all_news_nasdaq.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract only the stock of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only the stock symbol of interest from the external news dataset\n",
    "df_news_nasdaq_filtered = df_all_news_nasdaq.filter(\n",
    "    F.col(\"Stock_symbol\") == STOCK_SYMBOL_UPPER\n",
    ").orderBy(\"Date\")\n",
    "\n",
    "# Verify\n",
    "print(f\"Row count for df_news_nasdaq_filtered: {df_news_nasdaq_filtered.count()}\")\n",
    "df_news_nasdaq_filtered.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save a copy of the Nasdaq news dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify directories\n",
    "FOLDER_NASDAQ = \"stock_news/nasdaq\"\n",
    "TEMP_FOLDER_NASDAQ = f\"stock_news/nasdaq/nasdaq_{STOCK_SYMBOL_LOWER}\"\n",
    "\n",
    "# Write to a single CSV file\n",
    "df_news_nasdaq_filtered.coalesce(1).write.csv(\n",
    "    TEMP_FOLDER_NASDAQ, sep=\",\", header=True, mode=\"overwrite\"\n",
    ")\n",
    "\n",
    "# Move the part file to the desired filename\n",
    "for filename in os.listdir(TEMP_FOLDER_NASDAQ):\n",
    "    if filename.startswith(\"part-\"):\n",
    "        shutil.move(\n",
    "            os.path.join(TEMP_FOLDER_NASDAQ, filename),\n",
    "            os.path.join(FOLDER_NASDAQ, FILE_NAME_NASDAQ_NEWS),\n",
    "        )\n",
    "\n",
    "# Remove the temporary directory\n",
    "shutil.rmtree(TEMP_FOLDER_NASDAQ)\n",
    "\n",
    "local_variables = [\n",
    "    \"df_all_news_external\",\n",
    "    \"df_news_external_filtered\",\n",
    "]\n",
    "\n",
    "# Delete local variables if they exist\n",
    "for var in local_variables:\n",
    "    if var in locals():  # Check if the variable exists in the local scope\n",
    "        del locals()[var]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the News Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the external news and Nasdaq news datasets from CSV files to Spark dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read news data from CSV files to Spark dataframes\n",
    "df_external_news = spark.read.csv(\n",
    "    FILE_PATH_EXTERNAL_NEWS, header=True, schema=EXTERNAL_NEWS_SCHEMA\n",
    ")\n",
    "df_nasdaq_news = spark.read.csv(\n",
    "    FILE_PATH_NASDAQ_NEWS, header=True, schema=NASDAQ_NEWS_SCHEMA\n",
    ")\n",
    "\n",
    "# Store in memory\n",
    "df_external_news.persist()\n",
    "df_nasdaq_news.persist()\n",
    "\n",
    "# Verify the dataframes\n",
    "print(f\"Row count for df_external_news: {df_external_news.count()}\")\n",
    "df_external_news.show(5, truncate=True)\n",
    "\n",
    "print(f\"Row count for df_nasdaq_news: {df_nasdaq_news.count()}\")\n",
    "df_nasdaq_news.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop unused fields and merge the external and Nasdaq news datasets into one dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unused fields\n",
    "df_external_news = df_external_news.drop(\n",
    "    \"Luhn_summary\", \"Textrank_summary\", \"Lexrank_summary\"\n",
    ")\n",
    "df_nasdaq_news = df_nasdaq_news.drop(\n",
    "    \"Unnamed: 0\", \"Luhn_summary\", \"Textrank_summary\", \"Lexrank_summary\"\n",
    ")\n",
    "\n",
    "# Merge two dataframes\n",
    "df_news = df_nasdaq_news.unionByName(df_external_news)\n",
    "\n",
    "# Store in memory\n",
    "df_news.persist()\n",
    "\n",
    "# Verify\n",
    "# Expect count is 7419, where 2945 + 4474 = 7419\n",
    "print(f\"Row count for df_news: {df_news.count()}\")\n",
    "df_news.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the merged news dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the timestamps to UTC timezone. Example: Convert \"2019-01-15 00:00:00 UTC\" to \"2019-01-15 00:00:00\".\n",
    "df_news = df_news.withColumn(\n",
    "    \"Date\", F.to_utc_timestamp(F.to_timestamp(\"Date\", \"yyyy-MM-dd HH:mm:ss zzz\"), \"UTC\")\n",
    ").filter(F.col(\"Date\").isNotNull())\n",
    "\n",
    "# Add a \"Summarized\" field with all values set to 0.\n",
    "# Add a \"Sentiment_score\" field with all values set to 0.\n",
    "# Sort by Date field in descending order.\n",
    "df_news = (\n",
    "    df_news.withColumn(\"Summarized\", F.lit(0))\n",
    "    .withColumn(\"Sentiment_score\", F.lit(0))\n",
    "    .orderBy(\"Date\", ascending=False)\n",
    ")\n",
    "\n",
    "# Add a unique identifier field\n",
    "uuid_udf = F.udf(lambda: str(uuid.uuid4()), StringType())\n",
    "df_news = df_news.withColumn(\"UUID\", uuid_udf())\n",
    "\n",
    "# Verify\n",
    "print(f\"Row count for df_news: {df_news.count()}\")\n",
    "df_news.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save a copy of the preprocessed merged news dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify directories\n",
    "FOLDER_MERGED_NEWS = \"stock_news/merged\"\n",
    "TEMP_FOLDER_MERGED_NEWS = f\"stock_news/merged/news_{STOCK_SYMBOL_LOWER}\"\n",
    "\n",
    "# Write to a single CSV file\n",
    "df_news.coalesce(1).write.csv(\n",
    "    TEMP_FOLDER_MERGED_NEWS, sep=\",\", header=True, mode=\"overwrite\"\n",
    ")\n",
    "\n",
    "# Move the part file to the desired filename\n",
    "for filename in os.listdir(TEMP_FOLDER_MERGED_NEWS):\n",
    "    if filename.startswith(\"part-\"):\n",
    "        shutil.move(\n",
    "            os.path.join(TEMP_FOLDER_MERGED_NEWS, filename),\n",
    "            os.path.join(FOLDER_MERGED_NEWS, FILE_NAME_NEWS),\n",
    "        )\n",
    "\n",
    "# Remove the temporary directory\n",
    "shutil.rmtree(TEMP_FOLDER_MERGED_NEWS)\n",
    "\n",
    "# Uncache the dataframe in a non-blocking operation to free up memory.\n",
    "df_external_news.unpersist(blocking=False)\n",
    "df_nasdaq_news.unpersist(blocking=False)\n",
    "df_news.unpersist(blocking=False)\n",
    "\n",
    "local_variables = [\n",
    "    \"df_external_news\",\n",
    "    \"df_nasdaq_news\",\n",
    "    \"df_news\",\n",
    "    \"uuid_udf\",\n",
    "]\n",
    "\n",
    "# Delete local variables if they exist\n",
    "for var in local_variables:\n",
    "    if var in locals():  # Check if the variable exists in the local scope\n",
    "        del locals()[var]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize the News Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the merged news dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file to Spark dataframe\n",
    "df_merged_news = spark.read.csv(\n",
    "    FILE_PATH_MERGED_NEWS, header=True, schema=MERGED_NEWS_SCHEMA\n",
    ")\n",
    "\n",
    "# Store in memory\n",
    "df_merged_news.persist()\n",
    "\n",
    "# Verify\n",
    "print(f\"Row count for df_merged_news: {df_merged_news.count()}\")\n",
    "df_merged_news.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the news texts using a LSA Summarizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description:\n",
    "Takes the article text as input, parses it using PlaintextParser, and summarizes it using LsaSummarizer.\n",
    "\n",
    "Parameters:\n",
    "text (string): The news article.\n",
    "\n",
    "Returns:\n",
    "string: The summarized text.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def summarize_article(text):\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(LANGUAGE))\n",
    "\n",
    "    # Stemming reduces words to their root form for the summarizer to identify similar concepts expressed with\n",
    "    # different word forms.\n",
    "    stemmer = Stemmer(LANGUAGE)\n",
    "\n",
    "    # Initializes the summarizer with the stemmer\n",
    "    summarizer = LsaSummarizer(stemmer)\n",
    "\n",
    "    # Removes stop word to eliminate common non-keyword words.\n",
    "    summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "\n",
    "    # Generates the summarized text\n",
    "    summary = summarizer(parser.document, SENTENCES_COUNT)\n",
    "    return \" \".join([str(sentence) for sentence in summary])\n",
    "\n",
    "\n",
    "# A user-designed function to wrap the summarize_article function to be used in Spark.\n",
    "summarize_udf = F.udf(summarize_article, StringType())\n",
    "\n",
    "# Concats both fields with a period and space as the separator.\n",
    "# Execute the summarize_udf function\n",
    "df_merged_news = df_merged_news.withColumn(\n",
    "    \"Text\", F.concat_ws(\". \", \"Article_title\", \"Article\")\n",
    ").withColumn(\"Lsa_summary\", summarize_udf(\"Text\"))\n",
    "\n",
    "# Updated summarized indicator\n",
    "df_merged_news = df_merged_news.withColumn(\n",
    "    \"Summarized\",\n",
    "    F.when(\n",
    "        F.col(\"Lsa_summary\").isNotNull() & (F.col(\"Lsa_summary\") != \"\"), F.lit(1)\n",
    "    ).otherwise(0),\n",
    ")\n",
    "\n",
    "# Verify number of news with no summary\n",
    "# print(\n",
    "#     f\"Number of news with no summary: {df_merged_news.filter(col(\"Summarized\") == 0).count()}\"\n",
    "# )\n",
    "\n",
    "# Remove rows without a summary\n",
    "df_merged_news = df_merged_news.filter((F.col(\"Summarized\") == 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the results of the summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify\n",
    "print(f\"Row count for df_merged_news: {df_merged_news.count()}\")\n",
    "\n",
    "df_merged_news.select(\n",
    "    \"Date\",\n",
    "    \"Summarized\",\n",
    "    \"Text\",\n",
    "    \"Lsa_summary\",\n",
    ").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save a copy of the summarized news dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directories\n",
    "FOLDER_SUMMARIZED_NEWS = \"stock_news/summarized\"\n",
    "TEMP_FOLDER_SUMMARIZED_NEWS = \"stock_news/summarized/news_{STOCK_SYMBOL_LOWER}\"\n",
    "\n",
    "# Write to a single CSV file\n",
    "df_merged_news.coalesce(1).write.csv(\n",
    "    TEMP_FOLDER_SUMMARIZED_NEWS, sep=\",\", header=True, mode=\"overwrite\"\n",
    ")\n",
    "\n",
    "# Move the part file to the desired filename\n",
    "for filename in os.listdir(TEMP_FOLDER_SUMMARIZED_NEWS):\n",
    "    if filename.startswith(\"part-\"):\n",
    "        shutil.move(\n",
    "            os.path.join(TEMP_FOLDER_SUMMARIZED_NEWS, filename),\n",
    "            os.path.join(FOLDER_SUMMARIZED_NEWS, FILE_NAME_NEWS),\n",
    "        )\n",
    "\n",
    "# Remove the temporary directory\n",
    "shutil.rmtree(TEMP_FOLDER_SUMMARIZED_NEWS)\n",
    "\n",
    "# Uncache the dataframe in a non-blocking operation to free up memory.\n",
    "df_merged_news.unpersist(blocking=False)\n",
    "\n",
    "local_variables = [\n",
    "    \"df_merged_news\",\n",
    "    \"summarize_udf\",\n",
    "]\n",
    "\n",
    "# Delete local variables if they exist\n",
    "for var in local_variables:\n",
    "    if var in locals():  # Check if the variable exists in the local scope\n",
    "        del locals()[var]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the Sentiment Score of the News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the summarized news dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file to Spark dataframe\n",
    "df_summarized_news = spark.read.csv(\n",
    "    FILE_PATH_SUMMARIZED_NEWS, header=True, schema=SUMMARIZED_NEWS_SCHEMA\n",
    ")\n",
    "\n",
    "# Store in memory\n",
    "df_summarized_news.persist()\n",
    "\n",
    "# Verify\n",
    "print(f\"Row count for df_summarized_news: {df_summarized_news.count()}\")\n",
    "df_summarized_news.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the summaries and initialize the large-language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the UUID, Date, Url, and Lsa_summary from the first row\n",
    "collection = df_summarized_news.select(\n",
    "    F.collect_list(\"UUID\"),\n",
    "    F.collect_list(\"Date\"),\n",
    "    F.collect_list(\"Lsa_summary\"),\n",
    ").first()\n",
    "uuids = collection[0]\n",
    "dates = collection[1]\n",
    "lsa_summaries = collection[2]\n",
    "\n",
    "# Verify\n",
    "print(f\"Number of uuids: {len(uuids)}\")\n",
    "print(f\"Number of dates: {len(dates)}\")\n",
    "print(f\"Number of lsa_summaries: {len(lsa_summaries)}\")\n",
    "\n",
    "# Initializing the OllamaLLM\n",
    "try:\n",
    "    llm = ChatOllama(\n",
    "        model=MODEL, temperature=TEMPERATURE, num_predict=MAX_OUTPUT_TOKENS\n",
    "    )\n",
    "    print(\"ChatOllama instance created successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"Error creating ChatOllama instance:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed the summmmaries to the model in batches and capture the resulting sentiment score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holds the resulting output, i.e. the sentiment scores\n",
    "sentiments = []\n",
    "\n",
    "# Iterate in batches\n",
    "for i in range(0, len(lsa_summaries), BATCH_SIZE):\n",
    "    batch_summaries = lsa_summaries[i : i + BATCH_SIZE]\n",
    "    batch_uuids = uuids[i : i + BATCH_SIZE]\n",
    "\n",
    "    num_text = len(batch_summaries)\n",
    "    # print(f\"#{i}, num_text: {num_text}\")\n",
    "\n",
    "    batch_text = \" \".join(\n",
    "        [\n",
    "            f\"### {STOCK_SYMBOL_UPPER} Stock News: {summary} \"\n",
    "            for summary in batch_summaries\n",
    "        ]\n",
    "    )\n",
    "    # print(f\"#{i}, batch_text: {batch_text}\")\n",
    "\n",
    "    chat_template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                f\"Forget all previous instructions. You are a financial expert with stock recommendation experience. Based on news for a specific stock, provide a sentiment score in the range of 1 to 5 inclusive, where 1 is negative, 2 is somewhat negative, 3 is neutral, 4 is somewhat positive, and 5 is positive. {num_text} summerized news will be passed in each time. You will provide {num_text} scores, one score for each of the summerized news in the format as shown below in the response from the assistant.\",\n",
    "            ),\n",
    "            (\n",
    "                \"user\",\n",
    "                f\"### AAPL Stock News: Apple (AAPL) increased 22%. ### AAPL Stock News: Apple (AAPL) price decreased 30%. ### MSFT Stock News: Microsoft (MSTF) price has not changed. ### AAPL Stock News: Apple (AAPL) announced the new iPhone 15. ### AAPL Stock News: Apple (AAPL) will release the Vison Pro on Feb 2, 2024.\",\n",
    "            ),\n",
    "            (\"ai\", \"5, 1, 3, 4, 4\"),\n",
    "            (\"user\", batch_text),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    messages = chat_template.format_messages(num_text=num_text)\n",
    "    # print(f\"#{i}, messages: {messages}\")\n",
    "\n",
    "    response = llm.invoke(messages)\n",
    "    print(f\"#{i}: {response.content}\")\n",
    "\n",
    "    # Loop through each batch and append the sentiment scores to the list\n",
    "    for sentiment in response.content.split(\",\"):\n",
    "        stripped_sentiment = sentiment.strip()\n",
    "        try:\n",
    "            if stripped_sentiment:  # Check if the stripped sentiment is not empty\n",
    "                # Round the sentiment and clamp it between 1 and 5\n",
    "                rounded_sentiment = max(\n",
    "                    MIN_VALUE, min(MAX_VALUE, round(float(stripped_sentiment)))\n",
    "                )\n",
    "                sentiments.append(rounded_sentiment)\n",
    "        except ValueError:\n",
    "            print(f\"Invalid sentiment value: {stripped_sentiment}\")\n",
    "        # print(f\"#{i}, sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the sentiments to a file\n",
    "with open(FILE_PATH_SENTIMENT_SCORES, \"w\") as f:\n",
    "    f.writelines(f\"{sentiment}\\n\" for sentiment in sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify\n",
    "print(f\"Number of lsa_summaries: {len(lsa_summaries)}\")\n",
    "print(f\"Number of sentiments: {len(sentiments)}\")\n",
    "print(f\"Row count for df_summarized_news: {df_summarized_news.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO BE EXECUTED ONLY IF THE NUMBER OF SENTIMENTS DOES NOT MATCH THE NUMBER OF SUMMARIES\n",
    "# Step 1: Re-run the sentiment score for the affected batch\n",
    "# Step 2: Update the missing scores to the sentiments.csv file\n",
    "# Step 3: Verify the number of sentiments in the sentiments.csv file matches the number of summaries\n",
    "# Step 4: Execute code below to copy out the sentiments from sentiments.csv file to the sentiments list\n",
    "\n",
    "# sentiments = []\n",
    "# with open(FILE_PATH_SENTIMENT_SCORES, \"r\") as f:\n",
    "#     sentiments = [int(sentiment.strip()) for sentiment in f if sentiment.strip()]\n",
    "# print(f\"Number of sentiments: {len(sentiments)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the sentiment distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the count of each sentiment score\n",
    "sentiment_counts = {i: sentiments.count(i) for i in range(1, 6)}\n",
    "\n",
    "# Calculate the total count\n",
    "total_count = sum(sentiment_counts.values())\n",
    "\n",
    "# Create the bar chart\n",
    "plt.bar(sentiment_counts.keys(), sentiment_counts.values())\n",
    "plt.xlabel(\"Sentiment Score\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of the Sentiment Scores\")\n",
    "\n",
    "# Add percentage labels above each bar\n",
    "for i, count in enumerate(sentiment_counts.values()):\n",
    "    percentage = count / total_count * 100\n",
    "    plt.text(i + 1, count, f\"{percentage:.1f}%\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save a copy of the sentiment scored news dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(sentiments) != len(lsa_summaries):\n",
    "    raise ValueError(\"The number of sentiments does not match the number of summaries.\")\n",
    "\n",
    "# Combine lists into a list of tuples\n",
    "scored_news = list(zip(uuids, dates, sentiments, lsa_summaries))\n",
    "\n",
    "# Create a dataframe with the specified schema\n",
    "df_scored_news = spark.createDataFrame(scored_news, schema=SCORED_NEWS_SCHEMA)\n",
    "\n",
    "# Specify the directories\n",
    "FOLDER_SCORED_NEWS = \"stock_news/scored\"\n",
    "TEMP_FOLDER_SCORED_NEWS = \"stock_news/scored/news_{STOCK_SYMBOL_LOWER}\"\n",
    "\n",
    "# Write to a single CSV file\n",
    "df_scored_news.coalesce(1).write.csv(\n",
    "    TEMP_FOLDER_SCORED_NEWS, sep=\",\", header=True, mode=\"overwrite\"\n",
    ")\n",
    "\n",
    "# Move the part file to the desired filename\n",
    "for filename in os.listdir(TEMP_FOLDER_SCORED_NEWS):\n",
    "    if filename.startswith(\"part-\"):\n",
    "        shutil.move(\n",
    "            os.path.join(TEMP_FOLDER_SCORED_NEWS, filename),\n",
    "            os.path.join(FOLDER_SCORED_NEWS, FILE_NAME_NEWS),\n",
    "        )\n",
    "\n",
    "# Remove the temporary directory\n",
    "shutil.rmtree(TEMP_FOLDER_SCORED_NEWS)\n",
    "\n",
    "# Uncache the dataframe in a non-blocking operation to free up memory.\n",
    "df_summarized_news.unpersist(blocking=False)\n",
    "\n",
    "local_variables = [\n",
    "    \"collection\",\n",
    "    \"uuids\",\n",
    "    \"dates\",\n",
    "    \"lsa_summaries\",\n",
    "    \"sentiments\",\n",
    "    \"llm\",\n",
    "    \"chat_template\",\n",
    "    \"messages\",\n",
    "    \"response\",\n",
    "    \"scored_news\",\n",
    "    \"df_summarized_news\",\n",
    "    \"df_scored_news\",\n",
    "]\n",
    "\n",
    "# Delete local variables if they exist\n",
    "for var in local_variables:\n",
    "    if var in locals():  # Check if the variable exists in the local scope\n",
    "        del locals()[var]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exponentially Decay the Sentiment Score of the News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the sentiment scored stock news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scored_news = spark.read.csv(\n",
    "    FILE_PATH_SCORED_NEWS, header=True, schema=SCORED_NEWS_SCHEMA\n",
    ")\n",
    "\n",
    "# Verify\n",
    "print(f\"Row count for df_scored_news: {df_scored_news.count()}\")\n",
    "df_scored_news.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the average sentiment scores for each date. Then, populate the missing dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting only the date component and sort by date\n",
    "df_scored_news_copy = df_scored_news.select(\n",
    "    \"UUID\", F.to_date(\"Date\").alias(\"Date\"), \"Sentiment_score\", \"Lsa_summary\"\n",
    ").orderBy(\"Date\")\n",
    "\n",
    "# Group by \"Date\" and calculate the average \"Sentiment_score\"\n",
    "df_avg_sentiment = df_scored_news_copy.groupBy(\"Date\").agg(\n",
    "    F.avg(\"Sentiment_score\").alias(\"Sentiment_avg\")\n",
    ")\n",
    "\n",
    "# Retrieve the start and end date\n",
    "start_date = df_avg_sentiment.agg(F.min(\"Date\")).collect()[0][0]\n",
    "end_date = df_avg_sentiment.agg(F.max(\"Date\")).collect()[0][0]\n",
    "\n",
    "# Initialize a dataframe with the start and end dates\n",
    "df_date_range = spark.createDataFrame(\n",
    "    [(start_date, end_date)], [\"start_date\", \"end_date\"]\n",
    ")\n",
    "\n",
    "# Generate a date range using sequence\n",
    "df_dates = df_date_range.select(\n",
    "    F.expr(\"sequence(to_date(start_date), to_date(end_date), interval 1 day)\").alias(\n",
    "        \"Date\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Explode the array to get individual dates\n",
    "df_dates = df_dates.select(F.explode(\"Date\").alias(\"Date\"))\n",
    "\n",
    "# Join the complete date range with the original dataframe\n",
    "df_avg_sentiment_filled = df_dates.join(df_avg_sentiment, on=\"Date\", how=\"left\")\n",
    "\n",
    "# Store in memory\n",
    "df_avg_sentiment_filled.persist()\n",
    "\n",
    "# Verify\n",
    "print(f\"Row count for df_avg_sentiment_filled: {df_avg_sentiment_filled.count()}\")\n",
    "df_avg_sentiment_filled.show(df_avg_sentiment_filled.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply an exponential decay algorithm to the missing average sentiment scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a window specification to get the last valid sentiment and last valid date\n",
    "window_spec = Window.orderBy(\"Date\").rowsBetween(\n",
    "    Window.unboundedPreceding, Window.currentRow\n",
    ")\n",
    "\n",
    "# Get last valid sentiment\n",
    "df_avg_sentiment_filled = df_avg_sentiment_filled.withColumn(\n",
    "    \"Last_valid_sentiment\", F.last(\"Sentiment_avg\", ignorenulls=True).over(window_spec)\n",
    ")\n",
    "\n",
    "# Get last valid date only when sentiment is not null\n",
    "df_avg_sentiment_filled = df_avg_sentiment_filled.withColumn(\n",
    "    \"Last_valid_date\",\n",
    "    F.last(\n",
    "        F.when(F.col(\"Sentiment_avg\").isNotNull(), F.col(\"Date\")), ignorenulls=True\n",
    "    ).over(window_spec),\n",
    ")\n",
    "\n",
    "# Calculate the number of days since the last valid sentiment\n",
    "df_avg_sentiment_filled = df_avg_sentiment_filled.withColumn(\n",
    "    \"Days_since_last_valid\", (F.datediff(F.col(\"Date\"), F.col(\"Last_valid_date\")))\n",
    ")\n",
    "\n",
    "# Calculate decayed sentiment for rows where the average sentiment is null\n",
    "df_avg_sentiment_filled = df_avg_sentiment_filled.withColumn(\n",
    "    \"Decayed_sentiment\",\n",
    "    F.when(\n",
    "        F.col(\"Sentiment_avg\").isNull(),\n",
    "        BASE_VALUE\n",
    "        + (F.col(\"Last_valid_sentiment\") - BASE_VALUE)\n",
    "        * F.exp(-DECAY_RATE * F.col(\"Days_since_last_valid\")),\n",
    "    ).otherwise(F.col(\"Sentiment_avg\")),\n",
    ")\n",
    "\n",
    "# Round the DoubleType values to eight decimal places\n",
    "df_avg_sentiment_filled = (\n",
    "    df_avg_sentiment_filled.withColumn(\n",
    "        \"Sentiment_avg\", F.round(F.col(\"Sentiment_avg\"), 8)\n",
    "    )\n",
    "    .withColumn(\"Last_valid_sentiment\", F.round(F.col(\"Last_valid_sentiment\"), 8))\n",
    "    .withColumn(\"Decayed_sentiment\", F.round(F.col(\"Decayed_sentiment\"), 8))\n",
    ")\n",
    "\n",
    "# Verify\n",
    "print(f\"Row count for df_avg_sentiment_filled: {df_avg_sentiment_filled.count()}\")\n",
    "df_avg_sentiment_filled.show(df_avg_sentiment_filled.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save a copy of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directories\n",
    "FOLDER_DECAYED_NEWS = \"stock_news/decayed\"\n",
    "TEMP_FOLDER_DECAYED_NEWS = \"stock_news/decayed/news_{STOCK_SYMBOL_LOWER}\"\n",
    "\n",
    "# Write to a single CSV file\n",
    "df_avg_sentiment_filled.coalesce(1).write.csv(\n",
    "    TEMP_FOLDER_DECAYED_NEWS, sep=\",\", header=True, mode=\"overwrite\"\n",
    ")\n",
    "\n",
    "# Move the part file to the desired filename\n",
    "for filename in os.listdir(TEMP_FOLDER_DECAYED_NEWS):\n",
    "    if filename.startswith(\"part-\"):\n",
    "        shutil.move(\n",
    "            os.path.join(TEMP_FOLDER_DECAYED_NEWS, filename),\n",
    "            os.path.join(FOLDER_DECAYED_NEWS, FILE_NAME_NEWS),\n",
    "        )\n",
    "\n",
    "# Remove the temporary directory\n",
    "shutil.rmtree(TEMP_FOLDER_DECAYED_NEWS)\n",
    "\n",
    "# Uncache the dataframe in a non-blocking operation to free up memory.\n",
    "df_avg_sentiment_filled.unpersist(blocking=False)\n",
    "\n",
    "local_variables = [\n",
    "    \"window_spec\",\n",
    "    \"start_date\",\n",
    "    \"end_date\",\n",
    "    \"df_scored_news\",\n",
    "    \"df_scored_news_copy\",\n",
    "    \"df_avg_sentiment\",\n",
    "    \"df_date_range\",\n",
    "    \"df_dates\",\n",
    "    \"df_avg_sentiment_filled\",\n",
    "]\n",
    "\n",
    "# Delete local variables if they exist\n",
    "for var in local_variables:\n",
    "    if var in locals():  # Check if the variable exists in the local scope\n",
    "        del locals()[var]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrate the News and Price Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ingest the decayed sentiment dataset and the price dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read price data from CSV files to Spark dataframes\n",
    "df_decayed_sentiment = spark.read.csv(\n",
    "    FILE_PATH_DECAYED_NEWS, header=True, schema=DECAYED_SENTIMENT_SCHEMA\n",
    ")\n",
    "df_price = spark.read.csv(\n",
    "    FILE_PATH_PREPROCESSED_PRICE, header=True, schema=PREPROCESSED_PRICE_SCHEMA\n",
    ")\n",
    "\n",
    "# Verify\n",
    "print(f\"Row count for df_decayed_sentiment: {df_decayed_sentiment.count()}\")\n",
    "df_decayed_sentiment.printSchema()\n",
    "df_decayed_sentiment.show(5, truncate=True)\n",
    "\n",
    "print(f\"Row count for df_price: {df_price.count()}\")\n",
    "df_price.printSchema()\n",
    "df_price.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the price dataset with the average sentiment score taken from news dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left join the price data with the average sentiment data\n",
    "df_combined = df_price.join(df_decayed_sentiment, on=\"Date\", how=\"left\")\n",
    "\n",
    "# Store in memory\n",
    "df_combined.persist()\n",
    "\n",
    "# Drop unused fields\n",
    "df_combined = df_combined.drop(\n",
    "    \"Last_valid_sentiment\", \"Last_valid_date\", \"Days_since_last_valid\"\n",
    ")\n",
    "\n",
    "# Convert the decayed sentiment values to 3 if they are null\n",
    "df_combined = df_combined.withColumn(\n",
    "    \"Decayed_sentiment\",\n",
    "    F.when(\n",
    "        F.col(\"Decayed_sentiment\").isNull() | (F.col(\"Decayed_sentiment\") == \"\"),\n",
    "        BASE_VALUE,\n",
    "    ).otherwise(F.col(\"Decayed_sentiment\")),\n",
    ")\n",
    "\n",
    "# Verify\n",
    "df_combined.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the sentiments to between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the decayed sentiment\n",
    "df_combined = df_combined.withColumn(\n",
    "    \"Normalized_sentiment\",\n",
    "    (F.col(\"Decayed_sentiment\") - MIN_VALUE) / (MAX_VALUE - MIN_VALUE),\n",
    ")\n",
    "\n",
    "# Round the DoubleType values to eight decimal places\n",
    "df_combined = df_combined.withColumn(\n",
    "    \"Decayed_sentiment\", F.round(F.col(\"Decayed_sentiment\"), 8)\n",
    ").withColumn(\"Normalized_sentiment\", F.round(F.col(\"Normalized_sentiment\"), 8))\n",
    "\n",
    "# Verify\n",
    "df_combined.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save a copy of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directories\n",
    "FOLDER_COMBINED = \"lstm/data\"\n",
    "TEMP_FOLDER_COMBINED = f\"lstm/data/{STOCK_SYMBOL_LOWER}\"\n",
    "\n",
    "# Write to a single CSV file\n",
    "df_combined.coalesce(1).write.csv(\n",
    "    TEMP_FOLDER_COMBINED, sep=\",\", header=True, mode=\"overwrite\"\n",
    ")\n",
    "\n",
    "# Move the part file to the desired filename\n",
    "for filename in os.listdir(TEMP_FOLDER_COMBINED):\n",
    "    if filename.startswith(\"part-\"):\n",
    "        shutil.move(\n",
    "            os.path.join(TEMP_FOLDER_COMBINED, filename),\n",
    "            os.path.join(FOLDER_COMBINED, FILE_NAME_COMBINED),\n",
    "        )\n",
    "\n",
    "# Remove the temporary directory\n",
    "shutil.rmtree(TEMP_FOLDER_COMBINED)\n",
    "\n",
    "# Uncache the dataframe in a non-blocking operation to free up memory.\n",
    "df_combined.unpersist(blocking=False)\n",
    "\n",
    "local_variables = [\n",
    "    \"df_decayed_sentiment\",\n",
    "    \"df_price\",\n",
    "    \"df_combined\",\n",
    "]\n",
    "\n",
    "# Delete local variables if they exist\n",
    "for var in local_variables:\n",
    "    if var in locals():  # Check if the variable exists in the local scope\n",
    "        del locals()[var]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    def __init__(self):\n",
    "        self.start_date = None\n",
    "\n",
    "    def start(self):\n",
    "        self.start_date = dt.datetime.now()\n",
    "\n",
    "    def stop(self):\n",
    "        end_date = dt.datetime.now()\n",
    "        print(f\"Time taken: {end_date - self.start_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \"\"\"Load and transform data for the LSTM model.\"\"\"\n",
    "\n",
    "    def __init__(self, filename, split, cols, cols_to_norm, pred_len):\n",
    "        dataframe = pd.read_csv(filename)\n",
    "        i_split = int(len(dataframe) * split)\n",
    "        self.data_train = dataframe.get(cols).values[:i_split]\n",
    "        # print(f\"self.data_train: {self.data_train[:10]}\")\n",
    "        self.data_test = dataframe.get(cols).values[i_split:]\n",
    "        # print(f\"length of self.data_test: {len(self.data_test)}\")\n",
    "        # print(f\"self.data_test: {self.data_test}\")\n",
    "        self.cols_to_norm = cols_to_norm\n",
    "        self.pred_len = pred_len\n",
    "        self.len_train = len(self.data_train)\n",
    "        self.len_test = len(self.data_test)\n",
    "        self.len_train_windows = None\n",
    "\n",
    "    def get_test_data(self, seq_len, normalise, cols_to_norm):\n",
    "        \"\"\"\n",
    "        Create x, y test data windows\n",
    "        Warning: batch method, not generative, make sure you have enough memory to\n",
    "        load data, otherwise reduce size of the training split.\n",
    "        \"\"\"\n",
    "        data_windows = []\n",
    "        for i in range(self.len_test - seq_len):\n",
    "            data_windows.append(self.data_test[i : i + seq_len])\n",
    "\n",
    "        data_windows = np.array(data_windows).astype(float)\n",
    "        y_base = data_windows[:, 0, [0]]\n",
    "        # data_windows = self.normalise_windows(data_windows, single_window=False) if normalise else data_windows\n",
    "        data_windows = (\n",
    "            self.normalise_selected_columns(\n",
    "                data_windows, cols_to_norm, single_window=False\n",
    "            )\n",
    "            if normalise\n",
    "            else data_windows\n",
    "        )\n",
    "        cut_point = self.pred_len\n",
    "        # x = data_windows[:, :-cut_point:]\n",
    "        x = data_windows[:, :-1, :]\n",
    "        y = data_windows[:, -1, [0]]\n",
    "        return x, y, y_base\n",
    "\n",
    "    def get_train_data(self, seq_len, normalise):\n",
    "        \"\"\"\n",
    "        Create x, y train data windows\n",
    "        Warning: batch method, not generative, make sure you have enough memory to\n",
    "        load data, otherwise use generate_training_window() method.\n",
    "        \"\"\"\n",
    "        data_x = []\n",
    "        data_y = []\n",
    "        for i in range(self.len_train - seq_len):\n",
    "            x, y = self._next_window(i, seq_len, normalise)\n",
    "            data_x.append(x)\n",
    "            data_y.append(y)\n",
    "        return np.array(data_x), np.array(data_y)\n",
    "\n",
    "    def generate_train_batch(self, seq_len, batch_size, normalise):\n",
    "        \"\"\"Yield a generator of training data from filename on given list of cols split for train/test\"\"\"\n",
    "        i = 0\n",
    "        while i < (self.len_train - seq_len):\n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "            for b in range(batch_size):\n",
    "                if i >= (self.len_train - seq_len):\n",
    "                    # stop-condition for a smaller final batch if data doesn't divide evenly\n",
    "                    yield np.array(x_batch), np.array(y_batch)\n",
    "                    i = 0\n",
    "                x, y = self._next_window(i, seq_len, normalise)\n",
    "                x_batch.append(x)\n",
    "                y_batch.append(y)\n",
    "                i += 1\n",
    "            yield np.array(x_batch), np.array(y_batch)\n",
    "\n",
    "    def _next_window(self, i, seq_len, normalise):\n",
    "        \"\"\"Generates the next data window from the given index location i\"\"\"\n",
    "        window = self.data_train[i : i + seq_len]\n",
    "        # window = self.normalise_windows(window, single_window=True)[0] if normalise else window\n",
    "        window = (\n",
    "            self.normalise_selected_columns(\n",
    "                window, self.cols_to_norm, single_window=True\n",
    "            )[0]\n",
    "            if normalise\n",
    "            else window\n",
    "        )\n",
    "        # x = window[:-1]\n",
    "        x = window[:-1]\n",
    "        # y = window[0][2][0]\n",
    "        y = window[-1, [0]]\n",
    "        return x, y\n",
    "\n",
    "    def normalise_windows(self, window_data, single_window=False):\n",
    "        \"\"\"Normalise window with a base value of zero\"\"\"\n",
    "        normalised_data = []\n",
    "        window_data = [window_data] if single_window else window_data\n",
    "        for window in window_data:\n",
    "            normalised_window = []\n",
    "            for col_i in range(window.shape[1]):\n",
    "                w = window[0, col_i]\n",
    "                if w == 0:\n",
    "                    w = 1\n",
    "                normalised_col = [((float(p) / float(w)) - 1) for p in window[:, col_i]]\n",
    "                normalised_window.append(normalised_col)\n",
    "            normalised_window = (\n",
    "                np.array(normalised_window).T\n",
    "            )  # reshape and transpose array back into original multidimensional format\n",
    "            normalised_data.append(normalised_window)\n",
    "        return np.array(normalised_data)\n",
    "\n",
    "    # Modified normalization function to normalize only specific columns\n",
    "    def normalise_selected_columns(\n",
    "        self, window_data, columns_to_normalise, single_window=False\n",
    "    ):\n",
    "        normalised_data = []\n",
    "        window_data = [window_data] if single_window else window_data\n",
    "        for window in window_data:\n",
    "            normalised_window = []\n",
    "            for col_i in range(window.shape[1]):\n",
    "                if col_i in columns_to_normalise:\n",
    "                    # Normalize only if the column index is in the list of columns to normalize\n",
    "                    w = window[0, col_i]\n",
    "                    if w == 0:\n",
    "                        w = 1\n",
    "                    normalised_col = [\n",
    "                        ((float(p) / float(w)) - 1) for p in window[:, col_i]\n",
    "                    ]\n",
    "                else:\n",
    "                    # Keep the original data for columns not in the list\n",
    "                    normalised_col = window[:, col_i].tolist()\n",
    "                normalised_window.append(normalised_col)\n",
    "            normalised_window = np.array(normalised_window).T\n",
    "            normalised_data.append(normalised_window)\n",
    "        return np.array(normalised_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \"\"\"Build, train, and inference an LSTM model with Tensorflow Keras.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = Sequential()\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "        print(f\"MODEL: Loading model from file {filepath}\")\n",
    "        self.model = load_model(filepath)\n",
    "\n",
    "    def build_model(self, configs):\n",
    "        timer = Timer()\n",
    "        timer.start()\n",
    "\n",
    "        for layer in configs[\"model\"][\"layers\"]:\n",
    "            neurons = layer[\"neurons\"] if \"neurons\" in layer else None\n",
    "            dropout_rate = layer[\"rate\"] if \"rate\" in layer else None\n",
    "            activation = layer[\"activation\"] if \"activation\" in layer else None\n",
    "            return_seq = layer[\"return_seq\"] if \"return_seq\" in layer else None\n",
    "            input_timesteps = (\n",
    "                layer[\"input_timesteps\"] if \"input_timesteps\" in layer else None\n",
    "            )\n",
    "            input_dim = layer[\"input_dim\"] if \"input_dim\" in layer else None\n",
    "\n",
    "            if layer[\"type\"] == \"dense\":\n",
    "                self.model.add(Dense(neurons, activation=activation))\n",
    "            if layer[\"type\"] == \"lstm\":\n",
    "                self.model.add(\n",
    "                    LSTM(\n",
    "                        neurons,\n",
    "                        input_shape=(input_timesteps, input_dim),\n",
    "                        return_sequences=return_seq,\n",
    "                    )\n",
    "                )\n",
    "            if layer[\"type\"] == \"dropout\":\n",
    "                self.model.add(Dropout(dropout_rate))\n",
    "\n",
    "        self.model.compile(\n",
    "            loss=configs[\"model\"][\"loss\"], optimizer=configs[\"model\"][\"optimizer\"]\n",
    "        )\n",
    "\n",
    "        print(\"MODEL: Model Compiled\")\n",
    "        timer.stop()\n",
    "\n",
    "    def train(self, x, y, epochs, batch_size, save_dir):\n",
    "        timer = Timer()\n",
    "        timer.start()\n",
    "        print(\"MODEL: Training Started\")\n",
    "        print(f\"MODEL: epochs: {epochs}, batch size: {batch_size}\")\n",
    "\n",
    "        save_fname = os.path.join(\n",
    "            save_dir,\n",
    "            f\"{dt.datetime.now().strftime(\"%d%m%Y-%H%M%S\")}-e{str(epochs)}.keras\",\n",
    "        )\n",
    "\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor=\"val_loss\", patience=2),\n",
    "            ModelCheckpoint(\n",
    "                filepath=save_fname, monitor=\"val_loss\", save_best_only=True\n",
    "            ),\n",
    "        ]\n",
    "        self.model.fit(x, y, epochs=epochs, batch_size=batch_size, callbacks=callbacks)\n",
    "        self.model.save(save_fname)\n",
    "\n",
    "        print(f\"MODEL: Training Completed. Model saved as {save_fname}\")\n",
    "        timer.stop()\n",
    "\n",
    "    def train_generator(\n",
    "        self,\n",
    "        data_gen,\n",
    "        epochs,\n",
    "        batch_size,\n",
    "        steps_per_epoch,\n",
    "        save_dir,\n",
    "        sentiment_type,\n",
    "        model_name,\n",
    "        num_csvs,\n",
    "    ):\n",
    "        timer = Timer()\n",
    "        timer.start()\n",
    "        print(\"MODEL: Training Started\")\n",
    "        print(\n",
    "            f\"MODEL: epochs: {epochs}, batch size: {batch_size}, batches per epoch: {steps_per_epoch}\"\n",
    "        )\n",
    "        model_path = f\"{model_name}_{sentiment_type}_{num_csvs}.keras\"\n",
    "        save_fname = os.path.join(save_dir, model_path)\n",
    "\n",
    "        callbacks = [\n",
    "            ModelCheckpoint(filepath=save_fname, monitor=\"loss\", save_best_only=True)\n",
    "        ]\n",
    "        self.model.fit(\n",
    "            data_gen,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,\n",
    "        )\n",
    "\n",
    "        print(f\"MODEL: Training Completed. Model saved as {save_fname}\")\n",
    "        timer.stop()\n",
    "\n",
    "    def predict_sequences_multiple_modified(self, data, window_size, prediction_len):\n",
    "        \"\"\"\n",
    "        window_size = 50, prediction_len = 3\n",
    "        Pros: Computationally less expensive.\n",
    "        Cons: Less accurate\n",
    "        \"\"\"\n",
    "        print(\"MODEL: Predicting Sequences Multiple Modified...\")\n",
    "        prediction_seqs = []\n",
    "        for i in range(0, len(data), prediction_len):\n",
    "            curr_frame = data[i]\n",
    "            predicted = []\n",
    "            for j in range(prediction_len):\n",
    "                predicted.append(\n",
    "                    self.model.predict(curr_frame[newaxis, :, :], verbose=0)[0, 0]\n",
    "                )\n",
    "                curr_frame = curr_frame[1:]\n",
    "                curr_frame = np.insert(\n",
    "                    curr_frame, [window_size - 2], predicted[-1], axis=0\n",
    "                )\n",
    "            prediction_seqs.append(predicted)\n",
    "        return prediction_seqs\n",
    "\n",
    "    def predict_sequences_full(self, data, window_size):\n",
    "        \"\"\"\n",
    "        window_size = 50\n",
    "        Pros: By shifting the window one step at a time, it can effectively capture the context of the input data.\n",
    "        Cons: Computationally expensive to predict the entire dataset and will require more memory.\n",
    "        \"\"\"\n",
    "        # Shift the window by 1 new prediction each time, re-run predictions on new window\n",
    "        print(\"MODEL: Predicting Sequences Full...\")\n",
    "        curr_frame = data[0]\n",
    "        predicted = []\n",
    "        for i in range(len(data)):\n",
    "            predicted.append(\n",
    "                self.model.predict(curr_frame[newaxis, :, :], verbose=0)[0, 0]\n",
    "            )\n",
    "            curr_frame = curr_frame[1:]\n",
    "            curr_frame = np.insert(curr_frame, [window_size - 2], predicted[-1], axis=0)\n",
    "        return predicted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_TIME = dt.datetime.now().strftime(\"%Y%m%d%H\")\n",
    "# CURRENT_TIME = \"2024102709\"\n",
    "\n",
    "\n",
    "def output_results_and_errors_multiple(\n",
    "    predicted_data,\n",
    "    true_data,\n",
    "    true_data_base,\n",
    "    prediction_len,\n",
    "    file_name,\n",
    "    sentiment_type,\n",
    "    num_csvs,\n",
    "):\n",
    "    save_df = pd.DataFrame()\n",
    "\n",
    "    save_df[\"True_Data\"] = true_data.reshape(-1)\n",
    "    save_df[\"Base\"] = true_data_base.reshape(-1)\n",
    "\n",
    "    save_df[\"True_Data_origin\"] = (save_df[\"True_Data\"] + 1) * save_df[\"Base\"]\n",
    "\n",
    "    if predicted_data:\n",
    "        all_predicted_data = np.concatenate([p for p in predicted_data])\n",
    "    else:\n",
    "        all_predicted_data = predicted_data\n",
    "\n",
    "    file_name = file_name.split(\".\")[0]\n",
    "    sentiment_type = str(sentiment_type)\n",
    "\n",
    "    save_df[\"Predicted_Data\"] = pd.Series(all_predicted_data)\n",
    "\n",
    "    save_df[\"Predicted_Data_origin\"] = (save_df[\"Predicted_Data\"] + 1) * save_df[\"Base\"]\n",
    "\n",
    "    save_df = save_df.fillna(np.nan)\n",
    "    result_folder = f\"test_result_{num_csvs}\"\n",
    "    save_file_path = os.path.join(\n",
    "        result_folder,\n",
    "        f\"{file_name}_{sentiment_type}_{CURRENT_TIME}\",\n",
    "        f\"{file_name}_{sentiment_type}_{CURRENT_TIME}_predicted_data.csv\",\n",
    "    )\n",
    "\n",
    "    os.makedirs(\n",
    "        os.path.join(result_folder, f\"{file_name}_{sentiment_type}_{CURRENT_TIME}\"),\n",
    "        exist_ok=True,\n",
    "    )\n",
    "\n",
    "    save_df.to_csv(save_file_path, index=False)\n",
    "    print(f\"Data saved to {save_file_path}\")\n",
    "\n",
    "    min_length = min(len(save_df[\"Predicted_Data\"]), len(save_df[\"True_Data\"]))\n",
    "    predicted_data = save_df[\"Predicted_Data\"][:min_length]\n",
    "    true_data = save_df[\"True_Data\"][:min_length]\n",
    "\n",
    "    mae = mean_absolute_error(true_data, predicted_data)\n",
    "    mse = mean_squared_error(true_data, predicted_data)\n",
    "    r2 = r2_score(true_data, predicted_data)\n",
    "\n",
    "    print(f\"MAE: {mae}\")\n",
    "    print(f\"MSE: {mse}\")\n",
    "    print(f\"R: {r2}\")\n",
    "    results_df = pd.DataFrame({\"MAE\": [mae], \"MSE\": [mse], \"R2\": [r2]})\n",
    "\n",
    "    eval_file_path = os.path.join(\n",
    "        result_folder,\n",
    "        f\"{file_name}_{sentiment_type}_{CURRENT_TIME}\",\n",
    "        f\"{file_name}_{sentiment_type}_{CURRENT_TIME}_eval.csv\",\n",
    "    )\n",
    "\n",
    "    results_df.to_csv(eval_file_path, index=False)\n",
    "    print(f\"\\nResults saved to {eval_file_path}\")\n",
    "\n",
    "\n",
    "# Main Function\n",
    "def main(configs, data_filename, sentiment_type, flag_pred, model_name, num_csvs):\n",
    "    # print(\n",
    "    #     f\"flag_pred: {flag_pred}, sentiment_type: {sentiment_type}, data_filename: {data_filename}\"\n",
    "    # )\n",
    "    symbol_name = name.split(\".\")[0]\n",
    "    if not os.path.exists(configs[\"model\"][\"save_dir\"]):\n",
    "        os.makedirs(configs[\"model\"][\"save_dir\"])\n",
    "\n",
    "    data = DataLoader(\n",
    "        os.path.join(\"data\", data_filename),\n",
    "        configs[\"data\"][\"train_test_split\"],\n",
    "        configs[\"data\"][\"columns\"],\n",
    "        configs[\"data\"][\"columns_to_normalise\"],\n",
    "        configs[\"data\"][\"prediction_length\"],\n",
    "    )\n",
    "\n",
    "    model = Model()\n",
    "    model_path = f\"saved_models/{model_name}_{sentiment_type}_{num_csvs}.keras\"\n",
    "    if os.path.exists(model_path):\n",
    "        model.load_model(model_path)\n",
    "    else:\n",
    "        model.build_model(configs)\n",
    "\n",
    "    x, y = data.get_train_data(\n",
    "        seq_len=configs[\"data\"][\"sequence_length\"],\n",
    "        normalise=configs[\"data\"][\"normalise\"],\n",
    "    )\n",
    "    print(f\"X: {x.shape}\")\n",
    "    print(f\"Y: {y.shape}\")\n",
    "    \"\"\"\n",
    "\t# In-memory training\n",
    "\tmodel.train(\n",
    "\t\tx,\n",
    "\t\ty,\n",
    "\t\tepochs = configs['training']['epochs'],\n",
    "\t\tbatch_size = configs['training']['batch_size'],\n",
    "\t\tsave_dir = configs['model']['save_dir']\n",
    "\t)\n",
    "\t\"\"\"\n",
    "    # Out-of-memory training\n",
    "    steps_per_epoch = math.ceil(\n",
    "        (data.len_train - configs[\"data\"][\"sequence_length\"])\n",
    "        / configs[\"training\"][\"batch_size\"]\n",
    "    )\n",
    "    # print(\"======= End =======\")\n",
    "    model.train_generator(\n",
    "        data_gen=data.generate_train_batch(\n",
    "            seq_len=configs[\"data\"][\"sequence_length\"],\n",
    "            batch_size=configs[\"training\"][\"batch_size\"],\n",
    "            normalise=configs[\"data\"][\"normalise\"],\n",
    "        ),\n",
    "        epochs=configs[\"training\"][\"epochs\"],\n",
    "        batch_size=configs[\"training\"][\"batch_size\"],\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        save_dir=configs[\"model\"][\"save_dir\"],\n",
    "        sentiment_type=sentiment_type,\n",
    "        model_name=model_name,\n",
    "        num_csvs=num_csvs,\n",
    "    )\n",
    "    if flag_pred:\n",
    "        if symbol_name in pred_names:\n",
    "            print(\"-----Predicting-----\")\n",
    "            x_test, y_test, y_base = data.get_test_data(\n",
    "                seq_len=configs[\"data\"][\"sequence_length\"],\n",
    "                normalise=configs[\"data\"][\"normalise\"],\n",
    "                cols_to_norm=configs[\"data\"][\"columns_to_normalise\"],\n",
    "            )\n",
    "            print(\"Test Data:\")\n",
    "            print(f\"x_test.shape: {x_test.shape}\")\n",
    "            print(f\"y_test.shape: {y_test.shape}\")\n",
    "            predictions = model.predict_sequences_multiple_modified(\n",
    "                x_test,\n",
    "                configs[\"data\"][\"sequence_length\"],\n",
    "                configs[\"data\"][\"prediction_length\"],\n",
    "            )\n",
    "\n",
    "            output_results_and_errors_multiple(\n",
    "                predictions,\n",
    "                y_test,\n",
    "                y_base,\n",
    "                configs[\"data\"][\"prediction_length\"],\n",
    "                symbol_name,\n",
    "                sentiment_type,\n",
    "                num_csvs,\n",
    "            )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_name = \"lstm\"\n",
    "    sentiment_types = [\"sentiment\", \"non-sentiment\"]\n",
    "\n",
    "    # Test csvs = 5\n",
    "    names_1 = [\"aa.csv\"]\n",
    "    names_5 = [\"aa.csv\", \"aapl.csv\", \"amzn.csv\", \"msft.csv\", \"tsla.csv\"]\n",
    "    names_25 = []\n",
    "    names_50 = []\n",
    "    all_names = [names_1]\n",
    "    # all_names = [names_1, names_5, names_25, names_50]\n",
    "    pred_names = [\"aa\"]\n",
    "    for names in all_names:\n",
    "        num_stocks = len(names)\n",
    "        # For the first and second runs, only model training was performed\n",
    "        # In the third run, it will train and make predictions\n",
    "        for i in range(3):\n",
    "            if_pred = False\n",
    "            if i == 0 or i == 1:\n",
    "                continue\n",
    "            if i == 2:\n",
    "                if_pred = True\n",
    "            for sentiment_type in sentiment_types:\n",
    "                for name in names:\n",
    "                    configs = json.load(open(sentiment_type + \"-config.json\", \"r\"))\n",
    "                    print(f\"#{i}\")\n",
    "                    main(configs, name, sentiment_type, if_pred, model_name, num_stocks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_TEST_RESULT = \"lstm/test_result_5\"\n",
    "\n",
    "FILE_PATH_RESULTS_SENTI_PREDICTED = f\"{FOLDER_TEST_RESULT}/{STOCK_SYMBOL_LOWER}_sentiment_{CURRENT_TIME}/{STOCK_SYMBOL_LOWER}_sentiment_{CURRENT_TIME}_predicted_data.csv\"\n",
    "FILE_PATH_RESULTS_SENTI_EVAL = f\"{FOLDER_TEST_RESULT}/{STOCK_SYMBOL_LOWER}_sentiment_{CURRENT_TIME}/{STOCK_SYMBOL_LOWER}_sentiment_{CURRENT_TIME}_eval.csv\"\n",
    "FILE_PATH_RESULTS_NON_SENTI_PREDICTED = f\"{FOLDER_TEST_RESULT}/{STOCK_SYMBOL_LOWER}_non-sentiment_{CURRENT_TIME}/{STOCK_SYMBOL_LOWER}_non-sentiment_{CURRENT_TIME}_predicted_data.csv\"\n",
    "FILE_PATH_RESULTS_NON_SENTI_EVAL = f\"{FOLDER_TEST_RESULT}/{STOCK_SYMBOL_LOWER}_non-sentiment_{CURRENT_TIME}/{STOCK_SYMBOL_LOWER}_non-sentiment_{CURRENT_TIME}_eval.csv\"\n",
    "\n",
    "# Load the data from the files\n",
    "df_base_sentiment = spark.read.csv(\n",
    "    FILE_PATH_RESULTS_SENTI_PREDICTED,\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    ").select(\"True_Data_origin\", \"Predicted_Data_origin\")\n",
    "\n",
    "df_base_non_sentiment = spark.read.csv(\n",
    "    FILE_PATH_RESULTS_NON_SENTI_PREDICTED,\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    ").select(\"True_Data_origin\", \"Predicted_Data_origin\")\n",
    "\n",
    "# Collect the data from the PySpark DataFrame\n",
    "data_sentiment = df_base_sentiment.collect()\n",
    "data_non_sentiment = df_base_non_sentiment.collect()\n",
    "\n",
    "# Extract the data into separate lists\n",
    "true_data_origin = [row[\"True_Data_origin\"] for row in data_sentiment]\n",
    "predicted_data_origin_sentiment = [\n",
    "    row[\"Predicted_Data_origin\"] for row in data_sentiment\n",
    "]\n",
    "predicted_data_origin_non_sentiment = [\n",
    "    row[\"Predicted_Data_origin\"] for row in data_non_sentiment\n",
    "]\n",
    "\n",
    "# Plot the chart\n",
    "plt.plot(true_data_origin, label=\"Closing Price\", linestyle=\"-\")\n",
    "plt.plot(\n",
    "    predicted_data_origin_sentiment,\n",
    "    label=\"Predicted Closing Price with Sentiment\",\n",
    "    linestyle=\"-.\",\n",
    ")\n",
    "plt.plot(\n",
    "    predicted_data_origin_non_sentiment,\n",
    "    label=\"Predicted Closing Price without Sentiment\",\n",
    "    linestyle=\":\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.title(\n",
    "    f\"Stock {STOCK_SYMBOL_UPPER} - Predicted Closing Price Comparison between Sentiment and Non-Sentiment\"\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the files\n",
    "df_senti_eval = spark.read.csv(\n",
    "    FILE_PATH_RESULTS_SENTI_EVAL,\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    ")\n",
    "\n",
    "df_non_senti_eval = spark.read.csv(\n",
    "    FILE_PATH_RESULTS_NON_SENTI_EVAL,\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    ")\n",
    "\n",
    "# Calculate the evaluation metrics\n",
    "senti_mae = df_senti_eval.agg({\"MAE\": \"mean\"}).first()[\"avg(MAE)\"]\n",
    "senti_mse = df_senti_eval.agg({\"MSE\": \"mean\"}).first()[\"avg(MSE)\"]\n",
    "senti_r2 = df_senti_eval.agg({\"R2\": \"mean\"}).first()[\"avg(R2)\"]\n",
    "\n",
    "non_senti_mae = df_non_senti_eval.agg({\"MAE\": \"mean\"}).first()[\"avg(MAE)\"]\n",
    "non_senti_mse = df_non_senti_eval.agg({\"MSE\": \"mean\"}).first()[\"avg(MSE)\"]\n",
    "non_senti_r2 = df_non_senti_eval.agg({\"R2\": \"mean\"}).first()[\"avg(R2)\"]\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.bar([\"Sentiment\", \"Non-Sentiment\"], [senti_mae, non_senti_mae])\n",
    "plt.xlabel(\"Evaluation Type\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.title(\"MAE Comparison\")\n",
    "plt.text(0, senti_mae, str(round(senti_mae, 4)), ha=\"center\")\n",
    "plt.text(1, non_senti_mae, str(round(non_senti_mae, 4)), ha=\"center\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.bar([\"Sentiment\", \"Non-Sentiment\"], [senti_mse, non_senti_mse])\n",
    "plt.xlabel(\"Evaluation Type\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"MSE Comparison\")\n",
    "plt.text(0, senti_mse, str(round(senti_mse, 4)), ha=\"center\")\n",
    "plt.text(1, non_senti_mse, str(round(non_senti_mse, 4)), ha=\"center\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.bar([\"Sentiment\", \"Non-Sentiment\"], [senti_r2, non_senti_r2])\n",
    "plt.xlabel(\"Evaluation Type\")\n",
    "plt.ylabel(\"R2\")\n",
    "plt.title(\"R2 Comparison\")\n",
    "plt.text(0, senti_r2, str(round(senti_r2, 4)), ha=\"center\")\n",
    "plt.text(1, non_senti_r2, str(round(non_senti_r2, 4)), ha=\"center\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
