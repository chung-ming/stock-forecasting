{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the Python Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the necessary Python packages found in requirements.txt and import the necessary Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip3 install -r requirements.txt\n",
    "\n",
    "import nltk\n",
    "import os\n",
    "import shutil\n",
    "import uuid\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from pyspark.sql import SparkSession, Window, functions as F\n",
    "from pyspark.sql.types import (\n",
    "    DateType,\n",
    "    DecimalType,\n",
    "    IntegerType,\n",
    "    LongType,\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    TimestampType,\n",
    ")\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.utils import get_stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the Python notebook environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/24 20:43:04 WARN Utils: Your hostname, Mings-MBP.local resolves to a loopback address: 127.0.2.2; using 192.168.1.45 instead (on interface en0)\n",
      "24/10/24 20:43:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/24 20:43:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# A sentence tokenizer to split text into sentences\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "# Initialize a Spark session\n",
    "# spark = SparkSession.builder.appName(\"Stock Forecasting\").getOrCreate()\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Stock Forecasting\")\n",
    "    .config(\"spark.executor.memory\", \"4g\")\n",
    "    .config(\"spark.driver.memory\", \"2g\")\n",
    "    .config(\"spark.default.parallelism\", \"8\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Stock symbol\n",
    "STOCK_SYMBOL_UPPER = \"TSLA\"  # AA, AAPL, AMZN, MSFT, TSLA\n",
    "STOCK_SYMBOL_LOWER = \"tsla\"  # aa, aapl, amzn, msft, tsla\n",
    "\n",
    "# File names\n",
    "FILE_NAME_NEWS = f\"news_{STOCK_SYMBOL_LOWER}.csv\"\n",
    "FILE_NAME_PRICE = f\"price_{STOCK_SYMBOL_LOWER}.csv\"\n",
    "FILE_NAME_COMBINED = f\"combined_{STOCK_SYMBOL_LOWER}.csv\"\n",
    "\n",
    "# File paths\n",
    "FILE_PATH_EXTERNAL_NEWS = f\"stock_news/external/external_{STOCK_SYMBOL_LOWER}.csv\"\n",
    "FILE_PATH_NASDAQ_NEWS = f\"stock_news/nasdaq/nasdaq_{STOCK_SYMBOL_LOWER}.csv\"\n",
    "FILE_PATH_MERGED_NEWS = f\"stock_news/merged/{FILE_NAME_NEWS}\"\n",
    "FILE_PATH_SUMMARIZED_NEWS = f\"stock_news/summarized/{FILE_NAME_NEWS}\"\n",
    "FILE_PATH_SCORED_NEWS = f\"stock_news/scored/{FILE_NAME_NEWS}\"\n",
    "FILE_PATH_DECAYED_NEWS = f\"stock_news/decayed/{FILE_NAME_NEWS}\"\n",
    "FILE_PATH_SENTIMENT_SCORES = f\"stock_news/scored/sentiments_{STOCK_SYMBOL_LOWER}.csv\"\n",
    "FILE_PATH_FULL_HISTORY_PRICE = f\"stock_price/full_history/{STOCK_SYMBOL_UPPER}.csv\"\n",
    "FILE_PATH_PREPROCESSED_PRICE = f\"stock_price/preprocessed/{FILE_NAME_PRICE}\"\n",
    "\n",
    "# Used by the Tokenizer\n",
    "LANGUAGE = \"english\"  # Set the language\n",
    "\n",
    "# Used by the LSA Summarizer\n",
    "SENTENCES_COUNT = 3  # Set the max number of sentences in a summary\n",
    "\n",
    "# Used by the LLM\n",
    "# MODEL = \"llama3.2:1b-instruct-fp16\"\n",
    "MODEL = \"llama3.2:3b-instruct-fp16\"  # Set the large-language model\n",
    "BATCH_SIZE = 5  # Set the max number of sentences in a batch to be fed to the LLM\n",
    "TEMPERATURE = 0.0  # Set the temperature to 0.0 for deterministic output\n",
    "MAX_OUTPUT_TOKENS = 14  # Set the maximum number of output tokens\n",
    "\n",
    "# Used by LLM for sentiment scoring\n",
    "MIN_VALUE = 1  # The minimum value of the sentiment score\n",
    "BASE_VALUE = 3  # The midpoint between 1 and 5 of the sentiment score\n",
    "MAX_VALUE = 5  # The maximum value of the sentiment score\n",
    "\n",
    "# Used by exponential decay algorithm\n",
    "DECAY_RATE = 0.5  # Determines how quickly the sentiment decays over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the Price Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ingest the price dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count for df_price: 3399\n",
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "|date|                open|                high|                 low|               close|           adj close|   volume|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "|NULL|263.6600036621094000|265.1300048828125000|252.7100067138672000|253.1799926757812500|253.1799926757812500|113157100|\n",
      "|NULL|258.3500061035156000|263.3399963378906000|257.5199890136719000|261.4400024414062500|261.4400024414062500|106494400|\n",
      "|NULL|254.4900054931641000|257.9700012207031000|252.9100036621093200|256.6099853515625000|256.6099853515625000| 86892400|\n",
      "|NULL|256.7600098000000000|258.2200012207031000|251.3699951171875000|252.5399932861328000|252.5399932861328000| 93249800|\n",
      "|NULL|251.8999938964844000|254.8000030517578000|248.5500030517578000|254.5000000000000000|254.5000000000000000|109594200|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the schema\n",
    "price_schema = StructType(\n",
    "    [\n",
    "        StructField(\"date\", DateType(), True),\n",
    "        StructField(\"open\", DecimalType(24, 16), True),\n",
    "        StructField(\"high\", DecimalType(24, 16), True),\n",
    "        StructField(\"low\", DecimalType(24, 16), True),\n",
    "        StructField(\"close\", DecimalType(24, 16), True),\n",
    "        StructField(\"adj close\", DecimalType(24, 16), True),\n",
    "        StructField(\"volume\", LongType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Read price data from CSV files to Spark dataframes\n",
    "df_price = spark.read.csv(\n",
    "    FILE_PATH_FULL_HISTORY_PRICE, header=True, schema=price_schema\n",
    ")\n",
    "\n",
    "# Store in memory\n",
    "df_price.persist()\n",
    "\n",
    "# Verify\n",
    "print(f\"Row count for df_price: {df_price.count()}\")\n",
    "df_price.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the price dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count for df_price: 3399\n",
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "|Date|                Open|                High|                 Low|               Close|           Adj_close|   Volume|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "|NULL|263.6600036621094000|265.1300048828125000|252.7100067138672000|253.1799926757812500|253.1799926757812500|113157100|\n",
      "|NULL|258.3500061035156000|263.3399963378906000|257.5199890136719000|261.4400024414062500|261.4400024414062500|106494400|\n",
      "|NULL|254.4900054931641000|257.9700012207031000|252.9100036621093200|256.6099853515625000|256.6099853515625000| 86892400|\n",
      "|NULL|256.7600098000000000|258.2200012207031000|251.3699951171875000|252.5399932861328000|252.5399932861328000| 93249800|\n",
      "|NULL|251.8999938964844000|254.8000030517578000|248.5500030517578000|254.5000000000000000|254.5000000000000000|109594200|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rename the headers\n",
    "df_price = (\n",
    "    df_price.withColumnRenamed(\"date\", \"Date\")\n",
    "    .withColumnRenamed(\"open\", \"Open\")\n",
    "    .withColumnRenamed(\"high\", \"High\")\n",
    "    .withColumnRenamed(\"low\", \"Low\")\n",
    "    .withColumnRenamed(\"close\", \"Close\")\n",
    "    .withColumnRenamed(\"adj close\", \"Adj_close\")\n",
    "    .withColumnRenamed(\"volume\", \"Volume\")\n",
    "    .orderBy(\"Date\")\n",
    ")\n",
    "\n",
    "# Verify\n",
    "print(f\"Row count for df_price: {df_price.count()}\")\n",
    "df_price.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save a copy of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify directories\n",
    "FOLDER_PRICE = \"stock_price/preprocessed\"\n",
    "TEMP_FOLDER_PRICE = f\"stock_price/preprocessed/price_{STOCK_SYMBOL_LOWER}\"\n",
    "\n",
    "# Write to a single CSV file\n",
    "df_price.coalesce(1).write.csv(\n",
    "    TEMP_FOLDER_PRICE, sep=\",\", header=True, mode=\"overwrite\"\n",
    ")\n",
    "\n",
    "# Move the part file to the desired filename\n",
    "for filename in os.listdir(TEMP_FOLDER_PRICE):\n",
    "    if filename.startswith(\"part-\"):\n",
    "        shutil.move(\n",
    "            os.path.join(TEMP_FOLDER_PRICE, filename),\n",
    "            os.path.join(FOLDER_PRICE, FILE_NAME_PRICE),\n",
    "        )\n",
    "\n",
    "# Remove the temporary directory\n",
    "shutil.rmtree(TEMP_FOLDER_PRICE)\n",
    "\n",
    "# Uncache the dataframe in a non-blocking operation to free up memory.\n",
    "df_price.unpersist(blocking=False)\n",
    "\n",
    "# Delete the dataframe\n",
    "del df_price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the News Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the external news and Nasdaq news datasets from CSV files to Spark dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count for df_external_news: 1873\n",
      "+-------------------+--------------------+------------+--------------------+-----------------+------+-------+-----------+------------+----------------+---------------+\n",
      "|               Date|       Article_title|Stock_symbol|                 Url|        Publisher|Author|Article|Lsa_summary|Luhn_summary|Textrank_summary|Lexrank_summary|\n",
      "+-------------------+--------------------+------------+--------------------+-----------------+------+-------+-----------+------------+----------------+---------------+\n",
      "|2020-06-10 21:02:47|Tesla's Stock Clo...|        TSLA|https://www.benzi...|      Drew Levine|  NULL|   NULL|       NULL|        NULL|            NULL|           NULL|\n",
      "|2020-06-10 19:08:09|'Tesla factory wo...|        TSLA|https://www.benzi...|Benzinga Newsdesk|  NULL|   NULL|       NULL|        NULL|            NULL|           NULL|\n",
      "|2020-06-10 16:41:58|'Tesla hacker unl...|        TSLA|https://www.benzi...|Benzinga Newsdesk|  NULL|   NULL|       NULL|        NULL|            NULL|           NULL|\n",
      "|2020-06-10 15:33:18|GM On Track To Sp...|        TSLA|https://www.benzi...|Benzinga Newsdesk|  NULL|   NULL|       NULL|        NULL|            NULL|           NULL|\n",
      "|2020-06-10 14:15:07|Tesla's Journey T...|        TSLA|https://www.benzi...|     Wayne Duggan|  NULL|   NULL|       NULL|        NULL|            NULL|           NULL|\n",
      "+-------------------+--------------------+------------+--------------------+-----------------+------+-------+-----------+------------+----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Row count for df_nasdaq_news: 10573\n",
      "+----------+-------------------+--------------------+------------+--------------------+---------+------+--------------------+-----------+------------+----------------+---------------+\n",
      "|Unnamed: 0|               Date|       Article_title|Stock_symbol|                 Url|Publisher|Author|             Article|Lsa_summary|Luhn_summary|Textrank_summary|Lexrank_summary|\n",
      "+----------+-------------------+--------------------+------------+--------------------+---------+------+--------------------+-----------+------------+----------------+---------------+\n",
      "| 2129787.0|2023-12-17 06:00:00|Scotiabank's Mexi...|        TSLA|https://www.nasda...|     NULL|  NULL|    By Nivedita Balu|       NULL|        NULL|            NULL|           NULL|\n",
      "| 2129788.0|2023-12-17 04:00:00|Can the 'Magnific...|        TSLA|https://www.nasda...|     NULL|  NULL|                   M|       NULL|        NULL|            NULL|           NULL|\n",
      "| 2129789.0|2023-12-17 04:00:00|Notable ETF Outfl...|        TSLA|https://www.nasda...|     NULL|  NULL|Looking today at ...|       NULL|        NULL|            NULL|           NULL|\n",
      "| 2129790.0|2023-12-17 04:00:00|Swedes support Te...|        TSLA|https://www.nasda...|     NULL|  NULL|STOCKHOLM, Dec 18...|       NULL|        NULL|            NULL|           NULL|\n",
      "| 2129791.0|2023-12-17 04:00:00|Some Potential 20...|        TSLA|https://www.nasda...|     NULL|  NULL|It’s the time of ...|       NULL|        NULL|            NULL|           NULL|\n",
      "+----------+-------------------+--------------------+------------+--------------------+---------+------+--------------------+-----------+------------+----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the schemas\n",
    "external_schema = StructType(\n",
    "    [\n",
    "        StructField(\"Date\", TimestampType(), True),\n",
    "        StructField(\"Article_title\", StringType(), True),\n",
    "        StructField(\"Stock_symbol\", StringType(), True),\n",
    "        StructField(\"Url\", StringType(), True),\n",
    "        StructField(\"Publisher\", StringType(), True),\n",
    "        StructField(\"Author\", StringType(), True),\n",
    "        StructField(\"Article\", StringType(), True),\n",
    "        StructField(\"Lsa_summary\", StringType(), True),\n",
    "        StructField(\"Luhn_summary\", StringType(), True),\n",
    "        StructField(\"Textrank_summary\", StringType(), True),\n",
    "        StructField(\"Lexrank_summary\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "nasdaq_schema = StructType(\n",
    "    [\n",
    "        StructField(\"Unnamed: 0\", StringType(), True),\n",
    "        StructField(\"Date\", TimestampType(), True),\n",
    "        StructField(\"Article_title\", StringType(), True),\n",
    "        StructField(\"Stock_symbol\", StringType(), True),\n",
    "        StructField(\"Url\", StringType(), True),\n",
    "        StructField(\"Publisher\", StringType(), True),\n",
    "        StructField(\"Author\", StringType(), True),\n",
    "        StructField(\"Article\", StringType(), True),\n",
    "        StructField(\"Lsa_summary\", StringType(), True),\n",
    "        StructField(\"Luhn_summary\", StringType(), True),\n",
    "        StructField(\"Textrank_summary\", StringType(), True),\n",
    "        StructField(\"Lexrank_summary\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Read news data from CSV files to Spark dataframes\n",
    "df_external_news = spark.read.csv(\n",
    "    FILE_PATH_EXTERNAL_NEWS, header=True, schema=external_schema\n",
    ")\n",
    "df_nasdaq_news = spark.read.csv(\n",
    "    FILE_PATH_NASDAQ_NEWS, header=True, schema=nasdaq_schema\n",
    ")\n",
    "\n",
    "# Store in memory\n",
    "df_external_news.persist()\n",
    "df_nasdaq_news.persist()\n",
    "\n",
    "# Verify the dataframes\n",
    "print(f\"Row count for df_external_news: {df_external_news.count()}\")\n",
    "df_external_news.show(5, truncate=True)\n",
    "\n",
    "print(f\"Row count for df_nasdaq_news: {df_nasdaq_news.count()}\")\n",
    "df_nasdaq_news.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop unused fields and merge the external and Nasdaq news datasets into one dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count for df_news: 12446\n",
      "+-------------------+--------------------+------------+--------------------+---------+------+--------------------+-----------+\n",
      "|               Date|       Article_title|Stock_symbol|                 Url|Publisher|Author|             Article|Lsa_summary|\n",
      "+-------------------+--------------------+------------+--------------------+---------+------+--------------------+-----------+\n",
      "|2023-12-17 06:00:00|Scotiabank's Mexi...|        TSLA|https://www.nasda...|     NULL|  NULL|    By Nivedita Balu|       NULL|\n",
      "|2023-12-17 04:00:00|Can the 'Magnific...|        TSLA|https://www.nasda...|     NULL|  NULL|                   M|       NULL|\n",
      "|2023-12-17 04:00:00|Notable ETF Outfl...|        TSLA|https://www.nasda...|     NULL|  NULL|Looking today at ...|       NULL|\n",
      "|2023-12-17 04:00:00|Swedes support Te...|        TSLA|https://www.nasda...|     NULL|  NULL|STOCKHOLM, Dec 18...|       NULL|\n",
      "|2023-12-17 04:00:00|Some Potential 20...|        TSLA|https://www.nasda...|     NULL|  NULL|It’s the time of ...|       NULL|\n",
      "+-------------------+--------------------+------------+--------------------+---------+------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop unused fields\n",
    "df_external_news = df_external_news.drop(\n",
    "    \"Luhn_summary\", \"Textrank_summary\", \"Lexrank_summary\"\n",
    ")\n",
    "df_nasdaq_news = df_nasdaq_news.drop(\n",
    "    \"Unnamed: 0\", \"Luhn_summary\", \"Textrank_summary\", \"Lexrank_summary\"\n",
    ")\n",
    "\n",
    "# Merge two dataframes\n",
    "df_news = df_nasdaq_news.unionByName(df_external_news)\n",
    "\n",
    "# Store in memory\n",
    "df_news.persist()\n",
    "\n",
    "# Verify\n",
    "# Expect count is 7419, where 2945 + 4474 = 7419\n",
    "print(f\"Row count for df_news: {df_news.count()}\")\n",
    "df_news.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the merged news dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count for df_news: 12446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------+--------------------+---------+------+--------------------+-----------+----------+---------------+--------------------+\n",
      "|               Date|       Article_title|Stock_symbol|                 Url|Publisher|Author|             Article|Lsa_summary|Summarized|Sentiment_score|                UUID|\n",
      "+-------------------+--------------------+------------+--------------------+---------+------+--------------------+-----------+----------+---------------+--------------------+\n",
      "|2023-12-17 06:00:00|Scotiabank's Mexi...|        TSLA|https://www.nasda...|     NULL|  NULL|    By Nivedita Balu|       NULL|         0|              0|3722ea0b-b3c6-4b2...|\n",
      "|2023-12-17 04:00:00|Can the 'Magnific...|        TSLA|https://www.nasda...|     NULL|  NULL|                   M|       NULL|         0|              0|3021fc87-df9f-49f...|\n",
      "|2023-12-17 04:00:00|Notable ETF Outfl...|        TSLA|https://www.nasda...|     NULL|  NULL|Looking today at ...|       NULL|         0|              0|96627d86-cf36-49a...|\n",
      "|2023-12-17 04:00:00|Swedes support Te...|        TSLA|https://www.nasda...|     NULL|  NULL|STOCKHOLM, Dec 18...|       NULL|         0|              0|bccce611-f52b-4b8...|\n",
      "|2023-12-17 04:00:00|Some Potential 20...|        TSLA|https://www.nasda...|     NULL|  NULL|It’s the time of ...|       NULL|         0|              0|bfe83f44-3f38-4e4...|\n",
      "+-------------------+--------------------+------------+--------------------+---------+------+--------------------+-----------+----------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Standardize the timestamps to UTC timezone. Example: Convert \"2019-01-15 00:00:00 UTC\" to \"2019-01-15 00:00:00\".\n",
    "df_news = df_news.withColumn(\n",
    "    \"Date\", F.to_utc_timestamp(F.to_timestamp(\"Date\", \"yyyy-MM-dd HH:mm:ss zzz\"), \"UTC\")\n",
    ").filter(F.col(\"Date\").isNotNull())\n",
    "\n",
    "# Add a \"Summarized\" field with all values set to 0.\n",
    "# Add a \"Sentiment_score\" field with all values set to 0.\n",
    "# Sort by Date field in descending order.\n",
    "df_news = (\n",
    "    df_news.withColumn(\"Summarized\", F.lit(0))\n",
    "    .withColumn(\"Sentiment_score\", F.lit(0))\n",
    "    .orderBy(\"Date\", ascending=False)\n",
    ")\n",
    "\n",
    "# Add a unique identifier field\n",
    "uuid_udf = F.udf(lambda: str(uuid.uuid4()), StringType())\n",
    "df_news = df_news.withColumn(\"UUID\", uuid_udf())\n",
    "\n",
    "# Verify\n",
    "print(f\"Row count for df_news: {df_news.count()}\")\n",
    "df_news.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save a copy of the preprocessed merged news dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify directories\n",
    "FOLDER_MERGED_NEWS = \"stock_news/merged\"\n",
    "TEMP_FOLDER_MERGED_NEWS = f\"stock_news/merged/news_{STOCK_SYMBOL_LOWER}\"\n",
    "\n",
    "# Write to a single CSV file\n",
    "df_news.coalesce(1).write.csv(\n",
    "    TEMP_FOLDER_MERGED_NEWS, sep=\",\", header=True, mode=\"overwrite\"\n",
    ")\n",
    "\n",
    "# Move the part file to the desired filename\n",
    "for filename in os.listdir(TEMP_FOLDER_MERGED_NEWS):\n",
    "    if filename.startswith(\"part-\"):\n",
    "        shutil.move(\n",
    "            os.path.join(TEMP_FOLDER_MERGED_NEWS, filename),\n",
    "            os.path.join(FOLDER_MERGED_NEWS, FILE_NAME_NEWS),\n",
    "        )\n",
    "\n",
    "# Remove the temporary directory\n",
    "shutil.rmtree(TEMP_FOLDER_MERGED_NEWS)\n",
    "\n",
    "# Uncache the dataframe in a non-blocking operation to free up memory.\n",
    "df_external_news.unpersist(blocking=False)\n",
    "df_nasdaq_news.unpersist(blocking=False)\n",
    "df_news.unpersist(blocking=False)\n",
    "\n",
    "# Delete vairables\n",
    "del external_schema, nasdaq_schema, df_external_news, df_nasdaq_news, df_news, uuid_udf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize the News Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the merged news dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count for df_merged_news: 12446\n",
      "+-------------------+--------------------+------------+--------------------+---------+------+--------------------+-----------+----------+---------------+--------------------+\n",
      "|               Date|       Article_title|Stock_symbol|                 Url|Publisher|Author|             Article|Lsa_summary|Summarized|Sentiment_score|                UUID|\n",
      "+-------------------+--------------------+------------+--------------------+---------+------+--------------------+-----------+----------+---------------+--------------------+\n",
      "|2023-12-17 06:00:00|Scotiabank's Mexi...|        TSLA|https://www.nasda...|     NULL|  NULL|    By Nivedita Balu|       NULL|         0|              0|819d9d2d-e1e2-46a...|\n",
      "|2023-12-17 04:00:00|Can the 'Magnific...|        TSLA|https://www.nasda...|     NULL|  NULL|                   M|       NULL|         0|              0|bf343727-2394-477...|\n",
      "|2023-12-17 04:00:00|Notable ETF Outfl...|        TSLA|https://www.nasda...|     NULL|  NULL|Looking today at ...|       NULL|         0|              0|7cfac98c-8106-4b2...|\n",
      "|2023-12-17 04:00:00|Swedes support Te...|        TSLA|https://www.nasda...|     NULL|  NULL|STOCKHOLM, Dec 18...|       NULL|         0|              0|34d0d361-e430-43b...|\n",
      "|2023-12-17 04:00:00|Some Potential 20...|        TSLA|https://www.nasda...|     NULL|  NULL|It’s the time of ...|       NULL|         0|              0|11984b2b-0e44-45e...|\n",
      "+-------------------+--------------------+------------+--------------------+---------+------+--------------------+-----------+----------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the schemas\n",
    "merged_news_schema = StructType(\n",
    "    [\n",
    "        StructField(\"Date\", TimestampType(), True),\n",
    "        StructField(\"Article_title\", StringType(), True),\n",
    "        StructField(\"Stock_symbol\", StringType(), True),\n",
    "        StructField(\"Url\", StringType(), True),\n",
    "        StructField(\"Publisher\", StringType(), True),\n",
    "        StructField(\"Author\", StringType(), True),\n",
    "        StructField(\"Article\", StringType(), True),\n",
    "        StructField(\"Lsa_summary\", StringType(), True),\n",
    "        StructField(\"Summarized\", IntegerType(), True),\n",
    "        StructField(\"Sentiment_score\", IntegerType(), True),\n",
    "        StructField(\"UUID\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Read the CSV file to Spark dataframe\n",
    "df_merged_news = spark.read.csv(\n",
    "    FILE_PATH_MERGED_NEWS, header=True, schema=merged_news_schema\n",
    ")\n",
    "\n",
    "# Store in memory\n",
    "df_merged_news.persist()\n",
    "\n",
    "# Verify\n",
    "print(f\"Row count for df_merged_news: {df_merged_news.count()}\")\n",
    "df_merged_news.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the news texts using a LSA Summarizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description:\n",
    "Takes the article text as input, parses it using PlaintextParser, and summarizes it using LsaSummarizer.\n",
    "\n",
    "Parameters:\n",
    "text (string): The news article.\n",
    "\n",
    "Returns:\n",
    "string: The summarized text.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def summarize_article(text):\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(LANGUAGE))\n",
    "\n",
    "    # Stemming reduces words to their root form for the summarizer to identify similar concepts expressed with\n",
    "    # different word forms.\n",
    "    stemmer = Stemmer(LANGUAGE)\n",
    "\n",
    "    # Initializes the summarizer with the stemmer\n",
    "    summarizer = LsaSummarizer(stemmer)\n",
    "\n",
    "    # Removes stop word to eliminate common non-keyword words.\n",
    "    summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "\n",
    "    # Generates the summarized text\n",
    "    summary = summarizer(parser.document, SENTENCES_COUNT)\n",
    "    return \" \".join([str(sentence) for sentence in summary])\n",
    "\n",
    "\n",
    "# A user-designed function to wrap the summarize_article function to be used in Spark.\n",
    "summarize_udf = F.udf(summarize_article, StringType())\n",
    "\n",
    "# Concats both fields with a period and space as the separator.\n",
    "# Execute the summarize_udf function\n",
    "df_merged_news = df_merged_news.withColumn(\n",
    "    \"Text\", F.concat_ws(\". \", \"Article_title\", \"Article\")\n",
    ").withColumn(\"Lsa_summary\", summarize_udf(\"Text\"))\n",
    "\n",
    "# Updated summarized indicator\n",
    "df_merged_news = df_merged_news.withColumn(\n",
    "    \"Summarized\",\n",
    "    F.when(\n",
    "        F.col(\"Lsa_summary\").isNotNull() & (F.col(\"Lsa_summary\") != \"\"), F.lit(1)\n",
    "    ).otherwise(0),\n",
    ")\n",
    "\n",
    "# Verify number of news with no summary\n",
    "# print(\n",
    "#     f\"Number of news with no summary: {df_merged_news.filter(col(\"Summarized\") == 0).count()}\"\n",
    "# )\n",
    "\n",
    "# Remove rows without a summary\n",
    "df_merged_news = df_merged_news.filter((F.col(\"Summarized\") == 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the results of the summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ming/Codes/stock-forecasting/.venv/lib/python3.12/site-packages/sumy/summarizers/lsa.py:76: UserWarning: Number of words (1) is lower than number of sentences (2). LSA algorithm may not work properly.\n",
      "  warn(message % (words_count, sentences_count))\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count for df_merged_news: 12446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------+--------------------+---------+------+--------------------+--------------------+----------+---------------+--------------------+--------------------+\n",
      "|               Date|       Article_title|Stock_symbol|                 Url|Publisher|Author|             Article|         Lsa_summary|Summarized|Sentiment_score|                UUID|                Text|\n",
      "+-------------------+--------------------+------------+--------------------+---------+------+--------------------+--------------------+----------+---------------+--------------------+--------------------+\n",
      "|2023-12-17 06:00:00|Scotiabank's Mexi...|        TSLA|https://www.nasda...|     NULL|  NULL|    By Nivedita Balu|Scotiabank's Mexi...|         1|              0|819d9d2d-e1e2-46a...|Scotiabank's Mexi...|\n",
      "|2023-12-17 04:00:00|Can the 'Magnific...|        TSLA|https://www.nasda...|     NULL|  NULL|                   M|Can the 'Magnific...|         1|              0|bf343727-2394-477...|Can the 'Magnific...|\n",
      "|2023-12-17 04:00:00|Notable ETF Outfl...|        TSLA|https://www.nasda...|     NULL|  NULL|Looking today at ...|Notable ETF Outfl...|         1|              0|7cfac98c-8106-4b2...|Notable ETF Outfl...|\n",
      "|2023-12-17 04:00:00|Swedes support Te...|        TSLA|https://www.nasda...|     NULL|  NULL|STOCKHOLM, Dec 18...|Swedes support Te...|         1|              0|34d0d361-e430-43b...|Swedes support Te...|\n",
      "|2023-12-17 04:00:00|Some Potential 20...|        TSLA|https://www.nasda...|     NULL|  NULL|It’s the time of ...|Add those predict...|         1|              0|11984b2b-0e44-45e...|Some Potential 20...|\n",
      "+-------------------+--------------------+------------+--------------------+---------+------+--------------------+--------------------+----------+---------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 48:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Date               |Summarized|Text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |Lsa_summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
      "+-------------------+----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|2023-12-17 06:00:00|1         |Scotiabank's Mexico bet eyeing $1.6 trillion N.America trade is not without risks. By Nivedita Balu                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |Scotiabank's Mexico bet eyeing $1.6 trillion N.America trade is not without risks. By Nivedita Balu                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|2023-12-17 04:00:00|1         |Can the 'Magnificent Seven' Continue to Lead the Market Higher in 2024?. M                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |Can the 'Magnificent Seven' Continue to Lead the Market Higher in 2024?. M                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "|2023-12-17 04:00:00|1         |Notable ETF Outflow Detected - IWB, TSLA, LLY, JNJ. Looking today at week-over-week shares outstanding changes among the universe of ETFs covered at ETF Channel, one standout is the iShares Russell 1000 ETF (Symbol: IWB) where we have detected an approximate $584.9 million dollar outflow -- that's a 1.8% decrease week over week (from 125,400,000 to 123,150,000). Among the largest underlying components of IWB, in trading today Tesla Inc (Symbol: TSLA) is up about 1.8%, Eli Lilly (Symbol: LLY) is up about 1.9%, and Johnson & Johnson (Symbol: JNJ) is relatively unchanged. For a complete list of holdings, visit the IWB Holdings page » The chart below shows the one year price performance of IWB, versus its 200 day moving average:|Notable ETF Outflow Detected - IWB, TSLA, LLY, JNJ. Looking today at week-over-week shares outstanding changes among the universe of ETFs covered at ETF Channel, one standout is the iShares Russell 1000 ETF (Symbol: IWB) where we have detected an approximate $584.9 million dollar outflow -- that's a 1.8% decrease week over week (from 125,400,000 to 123,150,000). For a complete list of holdings, visit the IWB Holdings page » The chart below shows the one year price performance of IWB, versus its 200 day moving average:|\n",
      "|2023-12-17 04:00:00|1         |Swedes support Tesla mechanics' strike, poll shows. STOCKHOLM, Dec 18 (Reuters) - Most Swedes support an ongoing mechanics strike at Tesla's TSLA.O workshops in the Nordic country over the right to collective bargaining, an opinion poll by Novus showed.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |Swedes support Tesla mechanics' strike, poll shows. STOCKHOLM, Dec 18 (Reuters) - Most Swedes support an ongoing mechanics strike at Tesla's TSLA.O workshops in the Nordic country over the right to collective bargaining, an opinion poll by Novus showed.                                                                                                                                                                                                                                                                              |\n",
      "|2023-12-17 04:00:00|1         |Some Potential 2024 Winners Reside in QQQ, QQQM. It’s the time of year when investors are treated to a slew of market predictions for the next year. The onslaught of those offerings is dizzying. Add those predictions up and it’s likely market participants can absorb hundreds of stock picks for the year ahead. That's neither efficient nor practical for many investors to have portfolios populated in the dozens or hundreds. The good news is that some of the stocks market observers are most bullish on for 2024 are found in several familiar, cost-effective ETFs. Those include the Invesco QQQ Trust (QQQ) and Invesco NASDAQ 100 ETF (QQQM).                                                                                              |Add those predictions up and it’s likely market participants can absorb hundreds of stock picks for the year ahead. That's neither efficient nor practical for many investors to have portfolios populated in the dozens or hundreds. The good news is that some of the stocks market observers are most bullish on for 2024 are found in several familiar, cost-effective ETFs.                                                                                                                                                           |\n",
      "|2023-12-17 03:00:00|1         |Scotiabank's Mexico bet eyeing $1.6 trillion N.America trade is riddled with risks. TORONTO, Dec 18 (Reuters) - Bank of Nova Scotia (Scotiabank) BNS.TO is eying North America's booming $1.6 trillion trade with its renewed Mexico bet, a strategy that offers hope but brings risks that have seen many global lenders including Citigroup Inc. C.N scaling back.                                                                                                                                                                                                                                                                                                                                                                                          |Scotiabank's Mexico bet eyeing $1.6 trillion N.America trade is riddled with risks. TORONTO, Dec 18 (Reuters) - Bank of Nova Scotia (Scotiabank) BNS.TO is eying North America's booming $1.6 trillion trade with its renewed Mexico bet, a strategy that offers hope but brings risks that have seen many global lenders including Citigroup Inc. C.N scaling back.                                                                                                                                                                       |\n",
      "|2023-12-17 03:00:00|1         |3 Outstanding Stocks to Buy if Interest Rates Fall Next Year. There's reason to believe interest rates may decline in 2024, and that usually means it's a good idea to start investing in some interest-rate-sensitive stocks. In that line of thought, here's a look at why UPS (NYSE: UPS), Tesla (NASDAQ: TSLA), and machine vision company Cognex (NASDAQ: CGNX) are good ways to play this theme.                                                                                                                                                                                                                                                                                                                                                        |3 Outstanding Stocks to Buy if Interest Rates Fall Next Year. There's reason to believe interest rates may decline in 2024, and that usually means it's a good idea to start investing in some interest-rate-sensitive stocks. In that line of thought, here's a look at why UPS (NYSE: UPS), Tesla (NASDAQ: TSLA), and machine vision company Cognex (NASDAQ: CGNX) are good ways to play this theme.                                                                                                                                     |\n",
      "|2023-12-17 02:00:00|1         |EV Roundup: TSLA's Recall of 2M Vehicles, RIVN's Deal With AT&T & More. Electric vehicle (EV) behemoth Tesla TSLA is recalling more than 2 million vehicles on U.S. roads to install new safeguards in their Autopilot system.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |EV Roundup: TSLA's Recall of 2M Vehicles, RIVN's Deal With AT&T & More. Electric vehicle (EV) behemoth Tesla TSLA is recalling more than 2 million vehicles on U.S. roads to install new safeguards in their Autopilot system.                                                                                                                                                                                                                                                                                                             |\n",
      "|2023-12-17 02:00:00|1         |Pre-Market Most Active for Dec 18, 2023 : NIO, X, ARQT, CNHI, GOTU, SQQQ, TQQQ, TSLA, EBIX, VOD, CLF, ZIM. The NASDAQ 100 Pre-Market Indicator is up 5.92 to 16,629.37. The total Pre-Market volume is currently 50,840,748 shares traded.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |Pre-Market Most Active for Dec 18, 2023 : NIO, X, ARQT, CNHI, GOTU, SQQQ, TQQQ, TSLA, EBIX, VOD, CLF, ZIM. The NASDAQ 100 Pre-Market Indicator is up 5.92 to 16,629.37. The total Pre-Market volume is currently 50,840,748 shares traded.                                                                                                                                                                                                                                                                                                 |\n",
      "|2023-12-17 01:00:00|1         |1 Wall Street Analyst Thinks Tesla Stock Will Crash Nearly 50%. Should You Wait to Buy the Dip?. There's been no shortage of news and interest surrounding electric vehicle (EV) leader Tesla (NASDAQ: TSLA) this year. The stock has more than doubled so far in 2023. But one Wall Street analyst thinks the stock is heading back down to near where it began the year.                                                                                                                                                                                                                                                                                                                                                                                    |1 Wall Street Analyst Thinks Tesla Stock Will Crash Nearly 50%. There's been no shortage of news and interest surrounding electric vehicle (EV) leader Tesla (NASDAQ: TSLA) this year. But one Wall Street analyst thinks the stock is heading back down to near where it began the year.                                                                                                                                                                                                                                                  |\n",
      "+-------------------+----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/24 20:47:11 WARN PythonUDFRunner: Detected deadlock while completing task 0.0 in stage 48 (TID 47): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Verify\n",
    "print(f\"Row count for df_merged_news: {df_merged_news.count()}\")\n",
    "\n",
    "df_merged_news.show(5, truncate=True)\n",
    "\n",
    "df_merged_news.select(\n",
    "    \"Date\",\n",
    "    \"Summarized\",\n",
    "    \"Text\",\n",
    "    \"Lsa_summary\",\n",
    ").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save a copy of the summarized news dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ming/Codes/stock-forecasting/.venv/lib/python3.12/site-packages/sumy/summarizers/lsa.py:76: UserWarning: Number of words (1) is lower than number of sentences (2). LSA algorithm may not work properly.\n",
      "  warn(message % (words_count, sentences_count))\n",
      "/Users/ming/Codes/stock-forecasting/.venv/lib/python3.12/site-packages/sumy/summarizers/lsa.py:76: UserWarning: Number of words (1) is lower than number of sentences (2). LSA algorithm may not work properly.\n",
      "  warn(message % (words_count, sentences_count))\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Specify the directories\n",
    "FOLDER_SUMMARIZED_NEWS = \"stock_news/summarized\"\n",
    "TEMP_FOLDER_SUMMARIZED_NEWS = \"stock_news/summarized/news_{STOCK_SYMBOL_LOWER}\"\n",
    "\n",
    "# Write to a single CSV file\n",
    "df_merged_news.coalesce(1).write.csv(\n",
    "    TEMP_FOLDER_SUMMARIZED_NEWS, sep=\",\", header=True, mode=\"overwrite\"\n",
    ")\n",
    "\n",
    "# Move the part file to the desired filename\n",
    "for filename in os.listdir(TEMP_FOLDER_SUMMARIZED_NEWS):\n",
    "    if filename.startswith(\"part-\"):\n",
    "        shutil.move(\n",
    "            os.path.join(TEMP_FOLDER_SUMMARIZED_NEWS, filename),\n",
    "            os.path.join(FOLDER_SUMMARIZED_NEWS, FILE_NAME_NEWS),\n",
    "        )\n",
    "\n",
    "# Remove the temporary directory\n",
    "shutil.rmtree(TEMP_FOLDER_SUMMARIZED_NEWS)\n",
    "\n",
    "# Uncache the dataframe in a non-blocking operation to free up memory.\n",
    "df_merged_news.unpersist(blocking=False)\n",
    "\n",
    "# Delete variables\n",
    "del merged_news_schema, df_merged_news, summarize_udf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the Sentiment Score of the News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the summarized news dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count for df_summarized_news: 12446\n",
      "+-------------------+--------------------+------------+--------------------+---------+------+--------------------+--------------------+----------+---------------+--------------------+--------------------+\n",
      "|               Date|       Article_title|Stock_symbol|                 Url|Publisher|Author|             Article|         Lsa_summary|Summarized|Sentiment_score|                UUID|                Text|\n",
      "+-------------------+--------------------+------------+--------------------+---------+------+--------------------+--------------------+----------+---------------+--------------------+--------------------+\n",
      "|2023-12-17 06:00:00|Scotiabank's Mexi...|        TSLA|https://www.nasda...|     NULL|  NULL|    By Nivedita Balu|Scotiabank's Mexi...|         1|              0|819d9d2d-e1e2-46a...|Scotiabank's Mexi...|\n",
      "|2023-12-17 04:00:00|Can the 'Magnific...|        TSLA|https://www.nasda...|     NULL|  NULL|                   M|Can the 'Magnific...|         1|              0|bf343727-2394-477...|Can the 'Magnific...|\n",
      "|2023-12-17 04:00:00|Notable ETF Outfl...|        TSLA|https://www.nasda...|     NULL|  NULL|Looking today at ...|Notable ETF Outfl...|         1|              0|7cfac98c-8106-4b2...|Notable ETF Outfl...|\n",
      "|2023-12-17 04:00:00|Swedes support Te...|        TSLA|https://www.nasda...|     NULL|  NULL|STOCKHOLM, Dec 18...|Swedes support Te...|         1|              0|34d0d361-e430-43b...|Swedes support Te...|\n",
      "|2023-12-17 04:00:00|Some Potential 20...|        TSLA|https://www.nasda...|     NULL|  NULL|It’s the time of ...|Add those predict...|         1|              0|11984b2b-0e44-45e...|Some Potential 20...|\n",
      "+-------------------+--------------------+------------+--------------------+---------+------+--------------------+--------------------+----------+---------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the schemas\n",
    "summarized_news_schema = StructType(\n",
    "    [\n",
    "        StructField(\"Date\", TimestampType(), True),\n",
    "        StructField(\"Article_title\", StringType(), True),\n",
    "        StructField(\"Stock_symbol\", StringType(), True),\n",
    "        StructField(\"Url\", StringType(), True),\n",
    "        StructField(\"Publisher\", StringType(), True),\n",
    "        StructField(\"Author\", StringType(), True),\n",
    "        StructField(\"Article\", StringType(), True),\n",
    "        StructField(\"Lsa_summary\", StringType(), True),\n",
    "        StructField(\"Summarized\", IntegerType(), True),\n",
    "        StructField(\"Sentiment_score\", IntegerType(), True),\n",
    "        StructField(\"UUID\", StringType(), True),\n",
    "        StructField(\"Text\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Read the CSV file to Spark dataframe\n",
    "df_summarized_news = spark.read.csv(\n",
    "    FILE_PATH_SUMMARIZED_NEWS, header=True, schema=summarized_news_schema\n",
    ")\n",
    "\n",
    "# Store in memory\n",
    "df_summarized_news.persist()\n",
    "\n",
    "# Verify\n",
    "print(f\"Row count for df_summarized_news: {df_summarized_news.count()}\")\n",
    "df_summarized_news.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the summaries and initialize the large-language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of uuids: 12446\n",
      "Number of dates: 12446\n",
      "Number of lsa_summaries: 12446\n",
      "ChatOllama instance created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Get the UUID, Date, Url, and Lsa_summary from the first row\n",
    "collection = df_summarized_news.select(\n",
    "    F.collect_list(\"UUID\"),\n",
    "    F.collect_list(\"Date\"),\n",
    "    F.collect_list(\"Lsa_summary\"),\n",
    ").first()\n",
    "uuids = collection[0]\n",
    "dates = collection[1]\n",
    "lsa_summaries = collection[2]\n",
    "\n",
    "# Verify\n",
    "print(f\"Number of uuids: {len(uuids)}\")\n",
    "print(f\"Number of dates: {len(dates)}\")\n",
    "print(f\"Number of lsa_summaries: {len(lsa_summaries)}\")\n",
    "\n",
    "# Initializing the OllamaLLM\n",
    "try:\n",
    "    llm = ChatOllama(\n",
    "        model=MODEL, temperature=TEMPERATURE, num_predict=MAX_OUTPUT_TOKENS\n",
    "    )\n",
    "    print(\"ChatOllama instance created successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"Error creating ChatOllama instance:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed the summmmaries to the model in batches and capture the resulting sentiment score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0: 2, 3, 1, 5, 4\n",
      "#5: 2, 4, 3, 1, 1\n",
      "#10: 5, 5, 3, 4, 4\n",
      "#15: 5, 5, 3, 4, 2\n",
      "#20: 2, 4, 3, 1, 1\n",
      "#25: 4, 3, 3, 5, 5\n",
      "#30: 1, 3, 2, 5, 4\n",
      "#35: 2, 3, 4, 5, 1\n",
      "#40: 2, 1, 3, 4, 5\n",
      "#45: 4, 1, 3, 5, 5\n",
      "#50: 4, 5, 3, 4, 5\n",
      "#55: 2, 3, 4, 5, 1\n",
      "#60: 4, 2, 3, 5, 1\n",
      "#65: 2, 1, 3, 4, 3\n",
      "#70: 5, 4, 3, 2, 1\n",
      "#75: 4, 3, 2, 5, 1\n",
      "#80: 5, 4, 3, 4, 2\n",
      "#85: 4, 3, 2, 5, 1\n",
      "#90: 2, 3, 4, 5, 1\n",
      "#95: 5, 2, 3, 4, 1\n",
      "#100: 2, 1, 3, 5, 1\n",
      "#105: 2, 1, 3, 4, 5\n",
      "#110: 4, 1, 3, 5, 5\n",
      "#115: 2, 3, 4, 5, 1\n",
      "#120: 2, 2, 4, 5, 4\n",
      "#125: 4, 3, 3, 5, 5\n",
      "#130: 5, 2, 3, 4, 5\n",
      "#135: 4, 2, 3, 1, 5\n",
      "#140: 2, 1, 3, 2, 1\n",
      "#145: 2, 4, 3, 1, 1\n",
      "#150: 2, 3, 4, 1, 3\n",
      "#155: 2, 3, 4, 5, 1\n",
      "#160: 5, 4, 3, 4, 2\n",
      "#165: 2, 3, 3, 4, 1\n",
      "#170: 2, 3, 3, 4, 5\n",
      "#175: 5, 2, 3, 4, 1\n",
      "#180: 2, 3, 3, 4, 2\n",
      "#185: 4, 3, 3, 2, 5\n",
      "#190: 4, 5, 3, 2, 1\n",
      "#195: 2, 3, 4, 1, 1\n",
      "#200: 5, 4, 3, 2, 1\n",
      "#205: 4, 3, 5, 4, 5\n",
      "#210: 1, 3, 3, 4, 3\n",
      "#215: 2, 4, 3, 4, 3\n",
      "#220: 3, 5, 2, 4, 1\n",
      "#225: 1, 2, 3, 4, 5\n",
      "#230: 2, 3, 3, 5, 1\n",
      "#235: 2, 3, 4, 5, 1\n",
      "#240: 2, 3, 4, 1, 5\n",
      "#245: 2, 4, 3, 1, 1\n",
      "#250: 5, 4, 4, 5, 3\n",
      "#255: 4, 2, 1, 5, 3\n",
      "#260: 2, 2, 3, 1, 4\n",
      "#265: 4, 3, 3, 5, 5\n",
      "#270: 5, 4, 3, 4, 2\n",
      "#275: 2, 4, 3, 5, 1\n",
      "#280: 2, 3, 4, 1, 2\n",
      "#285: 2, 1, 3, 4, 5\n",
      "#290: 1, 4, 3, 2, 1\n",
      "#295: 2, 4, 3, 5, 1\n",
      "#300: 2, 3, 3, 4, 1\n",
      "#305: 2, 1, 3, 4, 5\n",
      "#310: 4, 3, 2, 5, 4\n",
      "#315: 5, 4, 3, 4, 5\n",
      "#320: 5, 5, 2, 4, 3\n",
      "#325: 2, 3, 3, 4, 5\n",
      "#330: 4, 3, 2, 5, 4\n",
      "#335: 2, 3, 3, 4, 2\n",
      "#340: 4, 3, 2, 5, 1\n",
      "#345: 1, 3, 3, 4, 5\n",
      "#350: 2, 1, 3, 4, 1\n",
      "#355: 4, 3, 2, 5, 1\n",
      "#360: 3, 4, 2, 5, 1\n",
      "#365: 2, 3, 3, 4, 5\n",
      "#370: 4, 3, 2, 5, 1\n",
      "#375: 4, 1, 3, 5, 2\n",
      "#380: 2, 1, 3, 4, 5\n",
      "#385: 2, 4, 3, 1, 1\n",
      "#390: 3, 4, 2, 1, 2\n",
      "#395: 5, 4, 3, 2, 1\n",
      "#400: 2, 3, 4, 1, 5\n",
      "#405: 2, 3, 4, 1, 3\n",
      "#410: 4, 2, 3, 4, 5\n",
      "#415: 4, 3, 2, 1, 5\n",
      "#420: 2, 3, 3, 1, 1\n",
      "#425: 2, 4, 3, 5, 1\n",
      "#430: 2, 3, 4, 1, 3\n",
      "#435: 2, 4, 3, 5, 1\n",
      "#440: 2, 4, 3, 1, 1\n",
      "#445: 5, 4, 3, 4, 5\n",
      "#450: 2, 4, 3, 1, 1\n",
      "#455: 4, 2, 3, 4, 5\n",
      "#460: 2, 1, 3, 4, 5\n",
      "#465: 2, 4, 3, 5, 4\n",
      "#470: 4, 2, 3, 5, 1\n",
      "#475: 4, 5, 3, 2, 1\n",
      "#480: 2, 4, 3, 5, 5\n",
      "#485: 2, 3, 4, 1, 5\n",
      "#490: 2, 3, 4, 4, 1\n",
      "#495: 2, 3, 4, 1, 1\n",
      "#500: 2, 3, 3, 4, 5\n",
      "#505: 4, 1, 3, 2, 4\n",
      "#510: 4, 3, 3, 5, 2\n",
      "#515: 4, 3, 3, 1, 2\n",
      "#520: 2, 3, 4, 1, 1\n",
      "#525: 2, 3, 4, 1, 3\n",
      "#530: 1, 3, 2, 4, 5\n",
      "#535: 4, 3, 4, 2, 4\n",
      "#540: 5, 5, 3, 4, 5\n",
      "#545: 1, 3, 3, 2, 1\n",
      "#550: 4, 3, 2, 5, 4\n",
      "#555: 4, 2, 3, 4, 1\n",
      "#560: 2, 3, 3, 4, 1\n",
      "#565: 4, 3, 2, 5, 1\n",
      "#570: 5, 1, 3, 4, 5\n",
      "#575: 2, 4, 3, 1, 5\n",
      "#580: 2, 3, 3, 4, 1\n",
      "#585: 2, 3, 3, 4, 5\n",
      "#590: 1, 3, 4, 5, 2\n",
      "#595: 5, 2, 3, 4, 5\n",
      "#600: 4, 2, 3, 4, 5\n",
      "#605: 5, 4, 3, 4, 2\n",
      "#610: 5, 4, 3, 2, 1\n",
      "#615: 1, 4, 3, 2, 1\n",
      "#620: 2, 1, 3, 4, 5\n",
      "#625: 1, 4, 3, 2, 5\n",
      "#630: 4, 3, 2, 1, 1\n",
      "#635: 2, 3, 4, 5, 1\n",
      "#640: 2, 3, 4, 1, 3\n",
      "#645: 2, 1, 3, 4, 5\n",
      "#650: 2, 1, 3, 1, 2\n",
      "#655: 4, 1, 5, 4, 2\n",
      "#660: 1, 3, 3, 4, 2\n",
      "#665: 2, 5, 3, 4, 1\n",
      "#670: 2, 4, 3, 1, 3\n",
      "#675: 4, 1, 3, 2, 3\n",
      "#680: 2, 3, 4, 1, 1\n",
      "#685: 2, 1, 3, 4, 5\n",
      "#690: 4, 3, 2, 5, 4\n",
      "#695: 2, 3, 3, 4, 2\n",
      "#700: 2, 1, 3, 4, 3\n",
      "#705: 2, 3, 4, 5, 1\n",
      "#710: 3, 1, 4, 5, 2\n",
      "#715: 2, 1, 3, 4, 5\n",
      "#720: 2, 2, 3, 4, 5\n",
      "#725: 4, 2, 3, 5, 1\n",
      "#730: 2, 3, 3, 4, 1\n",
      "#735: 2, 4, 3, 5, 4\n",
      "#740: 2, 5, 3, 4, 5\n",
      "#745: 2, 1, 3, 4, 5\n",
      "#750: 2, 3, 4, 5, 4\n",
      "#755: 2, 4, 3, 5, 1\n",
      "#760: 2, 4, 3, 5, 1\n",
      "#765: 2, 4, 1, 1, 3\n",
      "#770: 2, 5, 3, 4, 1\n",
      "#775: 2, 1, 3, 4, 5\n",
      "#780: 4, 2, 3, 5, 1\n",
      "#785: 2, 3, 4, 1, 3\n",
      "#790: 5, 1, 3, 2, 4\n",
      "#795: 2, 4, 3, 5, 1\n",
      "#800: 1, 4, 3, 5, 5\n",
      "#805: 1, 2, 3, 4, 1\n",
      "#810: 2, 1, 3, 4, 5\n",
      "#815: 2, 1, 3, 4, 5\n",
      "#820: 2, 3, 4, 1, 2\n",
      "#825: 2, 4, 3, 5, 3\n",
      "#830: 1, 2, 3, 4, 5\n",
      "#835: 2, 4, 3, 1, 5\n",
      "#840: 2, 4, 2, 1, 5\n",
      "#845: 4, 5, 2, 1, 3\n",
      "#850: 2, 3, 1, 2, 3\n",
      "#855: 1, 2, 3, 4, 1\n",
      "#860: 4, 1, 3, 2, 1\n",
      "#865: 4, 3, 2, 1, 5\n",
      "#870: 4, 3, 3, 2, 1\n",
      "#875: 4, 1, 3, 5, 5\n",
      "#880: 1, 2, 3, 4, 5\n",
      "#885: 2, 3, 3, 1, 1\n",
      "#890: 1, 2, 3, 4, 5\n",
      "#895: 2, 1, 3, 4, 1\n",
      "#900: 2, 3, 3, 1, 1\n",
      "#905: 2, 3, 3, 4, 1\n",
      "#910: 2, 4, 3, 1, 1\n",
      "#915: 1, 2, 3, 4, 1\n",
      "#920: 2, 1, 4, 3, 5\n",
      "#925: 3, 4, 5, 1, 4\n",
      "#930: 2, 1, 2, 4, 5\n",
      "#935: 2, 3, 1, 4, 5\n",
      "#940: 2, 4, 3, 5, 1\n",
      "#945: 2, 1, 4, 5, 3\n",
      "#950: 4, 3, 2, 1, 5\n",
      "#955: 4, 3, 5, 1, 3\n",
      "#960: 2, 4, 3, 5, 4\n",
      "#965: 4, 3, 3, 5, 2\n",
      "#970: 4, 2, 3, 4, 5\n",
      "#975: 2, 3, 4, 5, 4\n",
      "#980: 4, 3, 2, 5, 4\n",
      "#985: 2, 3, 3, 4, 1\n",
      "#990: 4, 3, 5, 2, 1\n",
      "#995: 2, 3, 3, 4, 5\n",
      "#1000: 5, 4, 3, 4, 2\n",
      "#1005: 1, 2, 3, 4, 5\n",
      "#1010: 2, 3, 4, 1, 5\n",
      "#1015: 4, 3, 2, 1, 1\n",
      "#1020: 2, 5, 3, 1, 1\n",
      "#1025: 2, 1, 3, 4, 5\n",
      "#1030: 2, 4, 3, 5, 1\n",
      "#1035: 2, 3, 3, 4, 5\n",
      "#1040: 2, 1, 3, 4, 5\n",
      "#1045: 2, 4, 3, 5, 1\n",
      "#1050: 2, 1, 3, 4, 5\n",
      "#1055: 2, 4, 3, 4, 1\n",
      "#1060: 4, 3, 2, 5, 3\n",
      "#1065: 5, 2, 3, 4, 5\n",
      "#1070: 2, 4, 3, 5, 1\n",
      "#1075: 1, 4, 3, 2, 1\n",
      "#1080: 5, 4, 3, 1, 2\n",
      "#1085: 2, 3, 4, 1, 1\n",
      "#1090: 1, 2, 3, 4, 1\n",
      "#1095: 4, 1, 5, 2, 5\n",
      "#1100: 2, 4, 3, 5, 5\n",
      "#1105: 2, 1, 3, 4, 1\n",
      "#1110: 2, 4, 3, 1, 1\n",
      "#1115: 1, 2, 3, 4, 5\n",
      "#1120: 4, 1, 3, 5, 2\n",
      "#1125: 4, 3, 2, 1, 3\n",
      "#1130: 2, 4, 3, 1, 2\n",
      "#1135: 2, 3, 1, 4, 3\n",
      "#1140: 5, 3, 3, 4, 2\n",
      "#1145: 2, 1, 3, 4, 5\n",
      "#1150: 2, 3, 4, 5, 1\n",
      "#1155: 3, 4, 2, 1, 3\n",
      "#1160: 2, 1, 3, 4, 2\n",
      "#1165: 1, 3, 4, 5, 4\n",
      "#1170: 5, 2, 4, 1, 3\n",
      "#1175: 2, 4, 3, 5, 1\n",
      "#1180: 1, 2, 3, 2, 1\n",
      "#1185: 2, 4, 3, 5, 4\n",
      "#1190: 2, 4, 3, 5, 5\n",
      "#1195: 4, 3, 2, 5, 1\n",
      "#1200: 2, 4, 3, 5, 1\n",
      "#1205: 5, 4, 3, 2, 1\n",
      "#1210: 2, 4, 3, 1, 2\n",
      "#1215: 4, 3, 3, 2, 1\n",
      "#1220: 5, 4, 2, 1, 1\n",
      "#1225: 4, 3, 5, 4, 3\n",
      "#1230: 2, 3, 4, 5, 1\n",
      "#1235: 2, 4, 3, 5, 1\n",
      "#1240: 4, 3, 5, 4, 5\n",
      "#1245: 2, 3, 3, 4, 1\n",
      "#1250: 4, 2, 3, 1, 5\n",
      "#1255: 4, 3, 2, 5, 4\n",
      "#1260: 2, 4, 1, 3, 2\n",
      "#1265: 1, 4, 3, 5, 4\n",
      "#1270: 4, 5, 3, 2, 5\n",
      "#1275: 2, 3, 4, 5, 1\n",
      "#1280: 2, 1, 3, 5, 2\n",
      "#1285: 4, 5, 3, 4, 1\n",
      "#1290: 5, 3, 3, 4, 2\n",
      "#1295: 2, 1, 3, 4, 5\n",
      "#1300: 2, 1, 3, 4, 5\n",
      "#1305: 4, 5, 3, 5, 1\n",
      "#1310: 5, 4, 3, 2, 1\n",
      "#1315: 2, 4, 3, 5, 1\n",
      "#1320: 2, 3, 3, 4, 1\n",
      "#1325: 2, 3, 3, 4, 5\n",
      "#1330: 2, 3, 4, 1, 1\n",
      "#1335: 4, 2, 3, 1, 1\n",
      "#1340: 2, 1, 3, 4, 5\n",
      "#1345: 2, 3, 3, 4, 1\n",
      "#1350: 2, 4, 3, 5, 1\n",
      "#1355: 5, 3, 2, 4, 4\n",
      "#1360: 2, 3, 4, 5, 1\n",
      "#1365: 1, 2, 3, 4, 5\n",
      "#1370: 4, 5, 2, 4, 5\n",
      "#1375: 2, 4, 3, 5, 1\n",
      "#1380: 2, 4, 3, 5, 1\n",
      "#1385: 5, 2, 3, 4, 5\n",
      "#1390: 2, 3, 4, 5, 5\n",
      "#1395: 4, 2, 3, 5, 1\n",
      "#1400: 2, 4, 3, 1, 1\n",
      "#1405: 1, 3, 4, 5, 2\n",
      "#1410: 4, 5, 3, 2, 1\n",
      "#1415: 2, 1, 3, 4, 2\n",
      "#1420: 2, 4, 3, 1, 2\n",
      "#1425: 4, 3, 3, 4, 5\n",
      "#1430: 5, 5, 3, 2, 1\n",
      "#1435: 1, 1, 4, 5, 4\n",
      "#1440: 5, 5, 3, 4, 1\n",
      "#1445: 5, 4, 3, 5, 5\n",
      "#1450: 5, 4, 3, 4, 4\n",
      "#1455: 2, 3, 3, 4, 5\n",
      "#1460: 2, 1, 3, 4, 5\n",
      "#1465: 4, 3, 3, 5, 5\n",
      "#1470: 2, 3, 4, 1, 5\n",
      "#1475: 2, 3, 3, 4, 5\n",
      "#1480: 1, 2, 3, 4, 5\n",
      "#1485: 1, 4, 3, 5, 5\n",
      "#1490: 2, 4, 3, 1, 5\n",
      "#1495: 2, 3, 4, 5, 1\n",
      "#1500: 2, 1, 3, 4, 3\n",
      "#1505: 4, 2, 3, 4, 5\n",
      "#1510: 2, 1, 3, 4, 3\n",
      "#1515: 5, 1, 3, 4, 2\n",
      "#1520: 2, 4, 3, 5, 1\n",
      "#1525: 2, 4, 3, 5, 1\n",
      "#1530: 5, 2, 3, 4, 5\n",
      "#1535: 4, 3, 5, 4, 2\n",
      "#1540: 4, 5, 3, 5, 5\n",
      "#1545: 2, 5, 4, 5, 3\n",
      "#1550: 4, 2, 3, 5, 5\n",
      "#1555: 4, 5, 3, 2, 1\n",
      "#1560: 2, 4, 3, 5, 4\n",
      "#1565: 4, 5, 1, 2, 4\n",
      "#1570: 5, 1, 3, 4, 2\n",
      "#1575: 4, 2, 3, 5, 1\n",
      "#1580: 2, 1, 3, 4, 5\n",
      "#1585: 2, 1, 3, 4, 1\n",
      "#1590: 3, 1, 3, 4, 5\n",
      "#1595: 1, 2, 3, 4, 2\n",
      "#1600: 2, 4, 3, 5, 4\n",
      "#1605: 2, 3, 4, 3, 1\n",
      "#1610: 1, 2, 3, 4, 5\n",
      "#1615: 2, 1, 3, 4, 3\n",
      "#1620: 2, 4, 3, 5, 1\n",
      "#1625: 2, 4, 3, 5, 5\n",
      "#1630: 3, 4, 3, 5, 5\n",
      "#1635: 2, 3, 3, 4, 1\n",
      "#1640: 2, 3, 4, 1, 2\n",
      "#1645: 2, 4, 3, 1, 1\n",
      "#1650: 2, 3, 1, 4, 5\n",
      "#1655: 3, 4, 4, 2, 1\n",
      "#1660: 1, 3, 4, 4, 5\n",
      "#1665: 4, 3, 3, 5, 5\n",
      "#1670: 2, 3, 4, 5, 1\n",
      "#1675: 4, 2, 3, 4, 1\n",
      "#1680: 2, 1, 3, 4, 5\n",
      "#1685: 2, 1, 3, 4, 5\n",
      "#1690: 2, 3, 4, 5, 1\n",
      "#1695: 4, 5, 3, 5, 1\n",
      "#1700: 2, 3, 4, 5, 1\n",
      "#1705: 4, 3, 2, 5, 4\n",
      "#1710: 4, 2, 3, 5, 1\n",
      "#1715: 1, 4, 3, 2, 1\n",
      "#1720: 2, 4, 3, 1, 1\n",
      "#1725: 2, 3, 3, 4, 5\n",
      "#1730: 2, 1, 3, 4, 5\n",
      "#1735: 2, 3, 3, 4, 1\n",
      "#1740: 4, 3, 2, 5, 5\n",
      "#1745: 1, 2, 3, 4, 5\n",
      "#1750: 5, 4, 3, 5, 5\n",
      "#1755: 4, 3, 2, 1, 3\n",
      "#1760: 4, 3, 3, 5, 1\n",
      "#1765: 2, 1, 3, 4, 5\n",
      "#1770: 4, 4, 5, 3, 2\n"
     ]
    }
   ],
   "source": [
    "# Holds the resulting output, i.e. the sentiment scores\n",
    "sentiments = []\n",
    "\n",
    "# Iterate in batches\n",
    "for i in range(0, len(lsa_summaries), BATCH_SIZE):\n",
    "    batch_summaries = lsa_summaries[i : i + BATCH_SIZE]\n",
    "    batch_uuids = uuids[i : i + BATCH_SIZE]\n",
    "\n",
    "    num_text = len(batch_summaries)\n",
    "    # print(f\"#{i}, num_text: {num_text}\")\n",
    "\n",
    "    batch_text = \" \".join(\n",
    "        [\n",
    "            f\"### {STOCK_SYMBOL_UPPER} Stock News: {summary} \"\n",
    "            for summary in batch_summaries\n",
    "        ]\n",
    "    )\n",
    "    # print(f\"#{i}, batch_text: {batch_text}\")\n",
    "\n",
    "    chat_template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                f\"Forget all previous instructions. You are a financial expert with stock recommendation experience. Based on news for a specific stock, provide a sentiment score in the range of 1 to 5 inclusive, where 1 is negative, 2 is somewhat negative, 3 is neutral, 4 is somewhat positive, and 5 is positive. {num_text} summerized news will be passed in each time. You will provide {num_text} scores, one score for each of the summerized news in the format as shown below in the response from the assistant.\",\n",
    "            ),\n",
    "            (\n",
    "                \"user\",\n",
    "                f\"### AAPL Stock News: Apple (AAPL) increased 22%. ### AAPL Stock News: Apple (AAPL) price decreased 30%. ### MSFT Stock News: Microsoft (MSTF) price has not changed. ### AAPL Stock News: Apple (AAPL) announced the new iPhone 15. ### AAPL Stock News: Apple (AAPL) will release the Vison Pro on Feb 2, 2024.\",\n",
    "            ),\n",
    "            (\"ai\", \"5, 1, 3, 4, 4\"),\n",
    "            (\"user\", batch_text),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    messages = chat_template.format_messages(num_text=num_text)\n",
    "    # print(f\"#{i}, messages: {messages}\")\n",
    "\n",
    "    response = llm.invoke(messages)\n",
    "    print(f\"#{i}: {response.content}\")\n",
    "\n",
    "    # Loop through each batch and append the sentiment scores to the list\n",
    "    sentiments.extend(\n",
    "        int(sentiment.strip()) for sentiment in response.content.split(\",\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the sentiments to a file\n",
    "with open(FILE_PATH_SENTIMENT_SCORES, \"w\") as f:\n",
    "    f.writelines(f\"{sentiment}\\n\" for sentiment in sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify\n",
    "print(f\"Number of lsa_summaries: {len(lsa_summaries)}\")\n",
    "print(f\"Number of sentiments: {len(sentiments)}\")\n",
    "print(f\"Row count for df_summarized_news: {df_summarized_news.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO BE EXECUTED ONLY IF THE NUMBER OF SENTIMENTS DOES NOT MATCH THE NUMBER OF SUMMARIES\n",
    "# Step 1: Re-run the sentiment score for the affected batch\n",
    "# Step 2: Update the missing scores to the sentiments.csv file\n",
    "# Step 3: Verify the number of sentiments in the sentiments.csv file matches the number of summaries\n",
    "# Step 4: Execute code below to copy out the sentiments from sentiments.csv file to the sentiments list\n",
    "\n",
    "# sentiment = []\n",
    "# with open(FILE_PATH_SENTIMENT_SCORES, \"r\") as f:\n",
    "#     sentiments = [int(sentiment.strip()) for sentiment in f if sentiment.strip()]\n",
    "# print(f\"Number of sentiments: {len(sentiments)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save a copy of the sentiment scored news dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(sentiments) != len(lsa_summaries):\n",
    "    raise ValueError(\"The number of sentiments does not match the number of summaries.\")\n",
    "\n",
    "# Define the schema\n",
    "scored_news_schema = StructType(\n",
    "    [\n",
    "        StructField(\"UUID\", StringType(), True),\n",
    "        StructField(\"Date\", TimestampType(), True),\n",
    "        StructField(\"Sentiment_score\", IntegerType(), True),\n",
    "        StructField(\"Lsa_summary\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Combine lists into a list of tuples\n",
    "scored_news = list(zip(uuids, dates, sentiments, lsa_summaries))\n",
    "\n",
    "# Create a dataframe with the specified schema\n",
    "df_scored_news = spark.createDataFrame(scored_news, schema=scored_news_schema)\n",
    "\n",
    "# Specify the directories\n",
    "FOLDER_SCORED_NEWS = \"stock_news/scored\"\n",
    "TEMP_FOLDER_SCORED_NEWS = \"stock_news/scored/news_{STOCK_SYMBOL_LOWER}\"\n",
    "\n",
    "# Write to a single CSV file\n",
    "df_scored_news.coalesce(1).write.csv(\n",
    "    TEMP_FOLDER_SCORED_NEWS, sep=\",\", header=True, mode=\"overwrite\"\n",
    ")\n",
    "\n",
    "# Move the part file to the desired filename\n",
    "for filename in os.listdir(TEMP_FOLDER_SCORED_NEWS):\n",
    "    if filename.startswith(\"part-\"):\n",
    "        shutil.move(\n",
    "            os.path.join(TEMP_FOLDER_SCORED_NEWS, filename),\n",
    "            os.path.join(FOLDER_SCORED_NEWS, FILE_NAME_NEWS),\n",
    "        )\n",
    "\n",
    "# Remove the temporary directory\n",
    "shutil.rmtree(TEMP_FOLDER_SCORED_NEWS)\n",
    "\n",
    "# Uncache the dataframe in a non-blocking operation to free up memory.\n",
    "df_summarized_news.unpersist(blocking=False)\n",
    "\n",
    "# Delete variables\n",
    "del (\n",
    "    summarized_news_schema,\n",
    "    scored_news_schema,\n",
    "    collection,\n",
    "    uuids,\n",
    "    dates,\n",
    "    lsa_summaries,\n",
    "    sentiments,\n",
    "    llm,\n",
    "    chat_template,\n",
    "    messages,\n",
    "    response,\n",
    "    scored_news,\n",
    "    df_summarized_news,\n",
    "    df_scored_news,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exponentially Decay the Sentiment Score of the News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the sentiment scored stock news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schemas\n",
    "scored_news_schema = StructType(\n",
    "    [\n",
    "        StructField(\"UUID\", StringType(), True),\n",
    "        StructField(\"Date\", TimestampType(), True),\n",
    "        StructField(\"Sentiment_score\", IntegerType(), True),\n",
    "        StructField(\"Lsa_summary\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "df_scored_news = spark.read.csv(\n",
    "    FILE_PATH_SCORED_NEWS, header=True, schema=scored_news_schema\n",
    ")\n",
    "\n",
    "# Verify\n",
    "print(f\"Row count for df_scored_news: {df_scored_news.count()}\")\n",
    "df_scored_news.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the average sentiment scores for each date. Then, populate the missing dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting only the date component and sort by date\n",
    "df_scored_news_copy = df_scored_news.select(\n",
    "    \"UUID\", F.to_date(\"Date\").alias(\"Date\"), \"Sentiment_score\", \"Lsa_summary\"\n",
    ").orderBy(\"Date\")\n",
    "\n",
    "# Group by \"Date\" and calculate the average \"Sentiment_score\"\n",
    "df_avg_sentiment = df_scored_news_copy.groupBy(\"Date\").agg(\n",
    "    F.avg(\"Sentiment_score\").alias(\"Sentiment_avg\")\n",
    ")\n",
    "\n",
    "# Retrieve the start and end date\n",
    "start_date = df_avg_sentiment.agg(F.min(\"Date\")).collect()[0][0]\n",
    "end_date = df_avg_sentiment.agg(F.max(\"Date\")).collect()[0][0]\n",
    "\n",
    "# Initialize a dataframe with the start and end dates\n",
    "df_date_range = spark.createDataFrame(\n",
    "    [(start_date, end_date)], [\"start_date\", \"end_date\"]\n",
    ")\n",
    "\n",
    "# Generate a date range using sequence\n",
    "df_dates = df_date_range.select(\n",
    "    F.expr(\"sequence(to_date(start_date), to_date(end_date), interval 1 day)\").alias(\n",
    "        \"Date\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Explode the array to get individual dates\n",
    "df_dates = df_dates.select(F.explode(\"Date\").alias(\"Date\"))\n",
    "\n",
    "# Join the complete date range with the original dataframe\n",
    "df_avg_sentiment_filled = df_dates.join(df_avg_sentiment, on=\"Date\", how=\"left\")\n",
    "\n",
    "# Store in memory\n",
    "df_avg_sentiment_filled.persist()\n",
    "\n",
    "# Verify\n",
    "print(f\"Row count for df_avg_sentiment_filled: {df_avg_sentiment_filled.count()}\")\n",
    "df_avg_sentiment_filled.show(df_avg_sentiment_filled.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply an exponential decay algorithm to the missing average sentiment scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a window specification to get the last valid sentiment and last valid date\n",
    "window_spec = Window.orderBy(\"Date\").rowsBetween(\n",
    "    Window.unboundedPreceding, Window.currentRow\n",
    ")\n",
    "\n",
    "# Get last valid sentiment\n",
    "df_avg_sentiment_filled = df_avg_sentiment_filled.withColumn(\n",
    "    \"Last_valid_sentiment\", F.last(\"Sentiment_avg\", ignorenulls=True).over(window_spec)\n",
    ")\n",
    "\n",
    "# Get last valid date only when sentiment is not null\n",
    "df_avg_sentiment_filled = df_avg_sentiment_filled.withColumn(\n",
    "    \"Last_valid_date\",\n",
    "    F.last(\n",
    "        F.when(F.col(\"Sentiment_avg\").isNotNull(), F.col(\"Date\")), ignorenulls=True\n",
    "    ).over(window_spec),\n",
    ")\n",
    "\n",
    "# Calculate the number of days since the last valid sentiment\n",
    "df_avg_sentiment_filled = df_avg_sentiment_filled.withColumn(\n",
    "    \"Days_since_last_valid\", (F.datediff(F.col(\"Date\"), F.col(\"Last_valid_date\")))\n",
    ")\n",
    "\n",
    "# Calculate decayed sentiment for rows where the average sentiment is null\n",
    "df_avg_sentiment_filled = df_avg_sentiment_filled.withColumn(\n",
    "    \"Decayed_sentiment\",\n",
    "    F.when(\n",
    "        F.col(\"Sentiment_avg\").isNull(),\n",
    "        BASE_VALUE\n",
    "        + (F.col(\"Last_valid_sentiment\") - BASE_VALUE)\n",
    "        * F.exp(-DECAY_RATE * F.col(\"Days_since_last_valid\")),\n",
    "    ).otherwise(F.col(\"Sentiment_avg\")),\n",
    ")\n",
    "\n",
    "# Verify\n",
    "print(f\"Row count for df_avg_sentiment_filled: {df_avg_sentiment_filled.count()}\")\n",
    "df_avg_sentiment_filled.show(df_avg_sentiment_filled.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save a copy of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directories\n",
    "FOLDER_DECAYED_NEWS = \"stock_news/decayed\"\n",
    "TEMP_FOLDER_DECAYED_NEWS = \"stock_news/decayed/news_{STOCK_SYMBOL_LOWER}\"\n",
    "\n",
    "# Write to a single CSV file\n",
    "df_avg_sentiment_filled.coalesce(1).write.csv(\n",
    "    TEMP_FOLDER_DECAYED_NEWS, sep=\",\", header=True, mode=\"overwrite\"\n",
    ")\n",
    "\n",
    "# Move the part file to the desired filename\n",
    "for filename in os.listdir(TEMP_FOLDER_DECAYED_NEWS):\n",
    "    if filename.startswith(\"part-\"):\n",
    "        shutil.move(\n",
    "            os.path.join(TEMP_FOLDER_DECAYED_NEWS, filename),\n",
    "            os.path.join(FOLDER_DECAYED_NEWS, FILE_NAME_NEWS),\n",
    "        )\n",
    "\n",
    "# Remove the temporary directory\n",
    "shutil.rmtree(TEMP_FOLDER_DECAYED_NEWS)\n",
    "\n",
    "# Uncache the dataframe in a non-blocking operation to free up memory.\n",
    "df_avg_sentiment_filled.unpersist(blocking=False)\n",
    "\n",
    "# Delete variables\n",
    "del (\n",
    "    window_spec,\n",
    "    start_date,\n",
    "    end_date,\n",
    "    df_scored_news,\n",
    "    df_scored_news_copy,\n",
    "    df_avg_sentiment,\n",
    "    df_date_range,\n",
    "    df_dates,\n",
    "    df_avg_sentiment_filled,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrate the News and Price Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ingest the decayed sentiment dataset and the price dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schemas\n",
    "decayed_sentiment_schema = StructType(\n",
    "    [\n",
    "        StructField(\"Date\", DateType(), True),\n",
    "        StructField(\"Sentiment_avg\", DecimalType(18, 16), True),\n",
    "        StructField(\"Last_valid_sentiment\", DecimalType(18, 16), True),\n",
    "        StructField(\"Last_valid_date\", DateType(), True),\n",
    "        StructField(\"Days_since_last_valid\", IntegerType(), True),\n",
    "        StructField(\"Decayed_sentiment\", DecimalType(18, 16), True),\n",
    "    ]\n",
    ")\n",
    "price_schema = StructType(\n",
    "    [\n",
    "        StructField(\"Date\", DateType(), True),\n",
    "        StructField(\"Open\", DecimalType(24, 16), True),\n",
    "        StructField(\"High\", DecimalType(24, 16), True),\n",
    "        StructField(\"Low\", DecimalType(24, 16), True),\n",
    "        StructField(\"Close\", DecimalType(24, 16), True),\n",
    "        StructField(\"Adj_close\", DecimalType(24, 16), True),\n",
    "        StructField(\"Volume\", LongType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Read price data from CSV files to Spark dataframes\n",
    "df_decayed_sentiment = spark.read.csv(\n",
    "    FILE_PATH_DECAYED_NEWS, header=True, schema=decayed_sentiment_schema\n",
    ")\n",
    "df_price = spark.read.csv(\n",
    "    FILE_PATH_PREPROCESSED_PRICE, header=True, schema=price_schema\n",
    ")\n",
    "\n",
    "# Verify\n",
    "print(f\"Row count for df_decayed_sentiment: {df_decayed_sentiment.count()}\")\n",
    "df_decayed_sentiment.show(5, truncate=True)\n",
    "print(f\"Row count for df_price: {df_price.count()}\")\n",
    "df_price.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the price dataset with the average sentiment score taken from news dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unused fields\n",
    "df_decayed_sentiment = df_decayed_sentiment.drop(\n",
    "    \"Sentiment_avg\", \"Last_valid_sentiment\", \"Last_valid_date\", \"Days_since_last_valid\"\n",
    ")\n",
    "\n",
    "# Join the price data with the average sentiment data\n",
    "df_combined = df_price.join(df_decayed_sentiment, on=\"Date\", how=\"left\")\n",
    "\n",
    "# Store in memory\n",
    "df_combined.persist()\n",
    "\n",
    "# Convert the decayed sentiment values to 3 if they are null\n",
    "df_combined = df_combined.withColumn(\n",
    "    \"Decayed_sentiment\",\n",
    "    F.when(\n",
    "        F.col(\"Decayed_sentiment\").isNull() | (F.col(\"Decayed_sentiment\") == \"\"),\n",
    "        BASE_VALUE,\n",
    "    ).otherwise(F.col(\"Decayed_sentiment\")),\n",
    ")\n",
    "\n",
    "# Verify\n",
    "df_combined.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the sentiments to between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the Decayed_sentiment column\n",
    "df_combined = df_combined.withColumn(\n",
    "    \"Normalized_sentiment\",\n",
    "    (F.col(\"Decayed_sentiment\") - MIN_VALUE) / (MAX_VALUE - MIN_VALUE),\n",
    ")\n",
    "\n",
    "# Verify\n",
    "df_combined.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save a copy of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directories\n",
    "FOLDER_COMBINED = \"stock_combined\"\n",
    "TEMP_FOLDER_COMBINED = \"stock_combined/{STOCK_SYMBOL_LOWER}\"\n",
    "\n",
    "# Write to a single CSV file\n",
    "df_combined.coalesce(1).write.csv(\n",
    "    TEMP_FOLDER_COMBINED, sep=\",\", header=True, mode=\"overwrite\"\n",
    ")\n",
    "\n",
    "# Move the part file to the desired filename\n",
    "for filename in os.listdir(TEMP_FOLDER_COMBINED):\n",
    "    if filename.startswith(\"part-\"):\n",
    "        shutil.move(\n",
    "            os.path.join(TEMP_FOLDER_COMBINED, filename),\n",
    "            os.path.join(FOLDER_COMBINED, FILE_NAME_COMBINED),\n",
    "        )\n",
    "\n",
    "# Remove the temporary directory\n",
    "shutil.rmtree(TEMP_FOLDER_COMBINED)\n",
    "\n",
    "# Uncache the dataframe in a non-blocking operation to free up memory.\n",
    "df_combined.unpersist(blocking=False)\n",
    "\n",
    "# Delete variables\n",
    "del decayed_sentiment_schema, price_schema, df_price, df_decayed_sentiment, df_combined"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
