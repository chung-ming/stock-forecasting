{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the Python Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the necessary Python packages found in requirements.txt and import the necessary Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip3 install -r requirements.txt\n",
    "\n",
    "import nltk\n",
    "import os\n",
    "import shutil\n",
    "import uuid\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from pyspark.sql import SparkSession, Window, functions as F\n",
    "from pyspark.sql.types import (\n",
    "    DateType,\n",
    "    DecimalType,\n",
    "    IntegerType,\n",
    "    LongType,\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    TimestampType,\n",
    ")\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.utils import get_stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the Python notebook environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A sentence tokenizer to split text into sentences\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "# Initialize a Spark session\n",
    "# spark = SparkSession.builder.appName(\"Stock Forecasting\").getOrCreate()\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Stock Forecasting\")\n",
    "    .config(\"spark.executor.memory\", \"4g\")\n",
    "    .config(\"spark.driver.memory\", \"2g\")\n",
    "    .config(\"spark.default.parallelism\", \"8\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Stock symbol\n",
    "STOCK_SYMBOL_UPPER = \"MSFT\"  # AA, AAPL, AMZN, MSFT, TSLA\n",
    "STOCK_SYMBOL_LOWER = \"msft\"  # aa, aapl, amzn, msft, tsla\n",
    "\n",
    "# File names\n",
    "FILE_NAME_NEWS = f\"news_{STOCK_SYMBOL_LOWER}.csv\"\n",
    "FILE_NAME_PRICE = f\"price_{STOCK_SYMBOL_LOWER}.csv\"\n",
    "FILE_NAME_COMBINED = f\"combined_{STOCK_SYMBOL_LOWER}.csv\"\n",
    "\n",
    "# File paths\n",
    "FILE_PATH_EXTERNAL_NEWS = f\"stock_news/external/external_{STOCK_SYMBOL_LOWER}.csv\"\n",
    "FILE_PATH_NASDAQ_NEWS = f\"stock_news/nasdaq/nasdaq_{STOCK_SYMBOL_LOWER}.csv\"\n",
    "FILE_PATH_MERGED_NEWS = f\"stock_news/merged/{FILE_NAME_NEWS}\"\n",
    "FILE_PATH_SUMMARIZED_NEWS = f\"stock_news/summarized/{FILE_NAME_NEWS}\"\n",
    "FILE_PATH_SCORED_NEWS = f\"stock_news/scored/{FILE_NAME_NEWS}\"\n",
    "FILE_PATH_DECAYED_NEWS = f\"stock_news/decayed/{FILE_NAME_NEWS}\"\n",
    "FILE_PATH_SENTIMENT_SCORES = f\"stock_news/scored/sentiments_{STOCK_SYMBOL_LOWER}.csv\"\n",
    "FILE_PATH_FULL_HISTORY_PRICE = f\"stock_price/full_history/{STOCK_SYMBOL_UPPER}.csv\"\n",
    "FILE_PATH_PREPROCESSED_PRICE = f\"stock_price/preprocessed/{FILE_NAME_PRICE}\"\n",
    "\n",
    "# Used by the Tokenizer\n",
    "LANGUAGE = \"english\"  # Set the language\n",
    "\n",
    "# Used by the LSA Summarizer\n",
    "SENTENCES_COUNT = 3  # Set the max number of sentences in a summary\n",
    "\n",
    "# Used by the LLM\n",
    "# MODEL = \"llama3.2:1b-instruct-fp16\"\n",
    "MODEL = \"llama3.2:3b-instruct-fp16\"  # Set the large-language model\n",
    "BATCH_SIZE = 5  # Set the max number of sentences in a batch to be fed to the LLM\n",
    "TEMPERATURE = 0.0  # Set the temperature to 0.0 for deterministic output\n",
    "MAX_OUTPUT_TOKENS = 14  # Set the maximum number of output tokens\n",
    "\n",
    "# Used by LLM for sentiment scoring\n",
    "MIN_VALUE = 1  # The minimum value of the sentiment score\n",
    "BASE_VALUE = 3  # The midpoint between 1 and 5 of the sentiment score\n",
    "MAX_VALUE = 5  # The maximum value of the sentiment score\n",
    "\n",
    "# Used by exponential decay algorithm\n",
    "DECAY_RATE = 0.5  # Determines how quickly the sentiment decays over time\n",
    "\n",
    "# Define the schema\n",
    "FULL_HISTORY_PRICE_SCHEMA = StructType(\n",
    "    [\n",
    "        StructField(\"date\", DateType(), True),\n",
    "        StructField(\"open\", DecimalType(24, 16), True),\n",
    "        StructField(\"high\", DecimalType(24, 16), True),\n",
    "        StructField(\"low\", DecimalType(24, 16), True),\n",
    "        StructField(\"close\", DecimalType(24, 16), True),\n",
    "        StructField(\"adj close\", DecimalType(24, 16), True),\n",
    "        StructField(\"volume\", LongType(), True),\n",
    "    ]\n",
    ")\n",
    "EXTERNAL_NEWS_SCHEMA = StructType(\n",
    "    [\n",
    "        StructField(\"Date\", TimestampType(), True),\n",
    "        StructField(\"Article_title\", StringType(), True),\n",
    "        StructField(\"Stock_symbol\", StringType(), True),\n",
    "        StructField(\"Url\", StringType(), True),\n",
    "        StructField(\"Publisher\", StringType(), True),\n",
    "        StructField(\"Author\", StringType(), True),\n",
    "        StructField(\"Article\", StringType(), True),\n",
    "        StructField(\"Lsa_summary\", StringType(), True),\n",
    "        StructField(\"Luhn_summary\", StringType(), True),\n",
    "        StructField(\"Textrank_summary\", StringType(), True),\n",
    "        StructField(\"Lexrank_summary\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "NASDAQ_NEWS_SCHEMA = StructType(\n",
    "    [\n",
    "        StructField(\"Unnamed: 0\", StringType(), True),\n",
    "        StructField(\"Date\", TimestampType(), True),\n",
    "        StructField(\"Article_title\", StringType(), True),\n",
    "        StructField(\"Stock_symbol\", StringType(), True),\n",
    "        StructField(\"Url\", StringType(), True),\n",
    "        StructField(\"Publisher\", StringType(), True),\n",
    "        StructField(\"Author\", StringType(), True),\n",
    "        StructField(\"Article\", StringType(), True),\n",
    "        StructField(\"Lsa_summary\", StringType(), True),\n",
    "        StructField(\"Luhn_summary\", StringType(), True),\n",
    "        StructField(\"Textrank_summary\", StringType(), True),\n",
    "        StructField(\"Lexrank_summary\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "MERGED_NEWS_SCHEMA = StructType(\n",
    "    [\n",
    "        StructField(\"Date\", TimestampType(), True),\n",
    "        StructField(\"Article_title\", StringType(), True),\n",
    "        StructField(\"Stock_symbol\", StringType(), True),\n",
    "        StructField(\"Url\", StringType(), True),\n",
    "        StructField(\"Publisher\", StringType(), True),\n",
    "        StructField(\"Author\", StringType(), True),\n",
    "        StructField(\"Article\", StringType(), True),\n",
    "        StructField(\"Lsa_summary\", StringType(), True),\n",
    "        StructField(\"Summarized\", IntegerType(), True),\n",
    "        StructField(\"Sentiment_score\", IntegerType(), True),\n",
    "        StructField(\"UUID\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "SUMMARIZED_NEWS_SCHEMA = StructType(\n",
    "    [\n",
    "        StructField(\"Date\", TimestampType(), True),\n",
    "        StructField(\"Article_title\", StringType(), True),\n",
    "        StructField(\"Stock_symbol\", StringType(), True),\n",
    "        StructField(\"Url\", StringType(), True),\n",
    "        StructField(\"Publisher\", StringType(), True),\n",
    "        StructField(\"Author\", StringType(), True),\n",
    "        StructField(\"Article\", StringType(), True),\n",
    "        StructField(\"Lsa_summary\", StringType(), True),\n",
    "        StructField(\"Summarized\", IntegerType(), True),\n",
    "        StructField(\"Sentiment_score\", IntegerType(), True),\n",
    "        StructField(\"UUID\", StringType(), True),\n",
    "        StructField(\"Text\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "SCORED_NEWS_SCHEMA = StructType(\n",
    "    [\n",
    "        StructField(\"UUID\", StringType(), True),\n",
    "        StructField(\"Date\", TimestampType(), True),\n",
    "        StructField(\"Sentiment_score\", IntegerType(), True),\n",
    "        StructField(\"Lsa_summary\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "DECAYED_SENTIMENT_SCHEMA = StructType(\n",
    "    [\n",
    "        StructField(\"Date\", DateType(), True),\n",
    "        StructField(\"Sentiment_avg\", DecimalType(18, 16), True),\n",
    "        StructField(\"Last_valid_sentiment\", DecimalType(18, 16), True),\n",
    "        StructField(\"Last_valid_date\", DateType(), True),\n",
    "        StructField(\"Days_since_last_valid\", IntegerType(), True),\n",
    "        StructField(\"Decayed_sentiment\", DecimalType(18, 16), True),\n",
    "    ]\n",
    ")\n",
    "PREPROCESSED_PRICE_SCHEMA = StructType(\n",
    "    [\n",
    "        StructField(\"Date\", DateType(), True),\n",
    "        StructField(\"Open\", DecimalType(24, 16), True),\n",
    "        StructField(\"High\", DecimalType(24, 16), True),\n",
    "        StructField(\"Low\", DecimalType(24, 16), True),\n",
    "        StructField(\"Close\", DecimalType(24, 16), True),\n",
    "        StructField(\"Adj_close\", DecimalType(24, 16), True),\n",
    "        StructField(\"Volume\", LongType(), True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the Price Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ingest the price dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read price data from CSV files to Spark dataframes\n",
    "df_price = spark.read.csv(\n",
    "    FILE_PATH_FULL_HISTORY_PRICE, header=True, schema=FULL_HISTORY_PRICE_SCHEMA\n",
    ")\n",
    "\n",
    "# Store in memory\n",
    "df_price.persist()\n",
    "\n",
    "# Verify\n",
    "print(f\"Row count for df_price: {df_price.count()}\")\n",
    "df_price.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the price dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the headers\n",
    "df_price = (\n",
    "    df_price.withColumnRenamed(\"date\", \"Date\")\n",
    "    .withColumnRenamed(\"open\", \"Open\")\n",
    "    .withColumnRenamed(\"high\", \"High\")\n",
    "    .withColumnRenamed(\"low\", \"Low\")\n",
    "    .withColumnRenamed(\"close\", \"Close\")\n",
    "    .withColumnRenamed(\"adj close\", \"Adj_close\")\n",
    "    .withColumnRenamed(\"volume\", \"Volume\")\n",
    "    .orderBy(\"Date\")\n",
    ")\n",
    "\n",
    "# Verify\n",
    "print(f\"Row count for df_price: {df_price.count()}\")\n",
    "df_price.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save a copy of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify directories\n",
    "FOLDER_PRICE = \"stock_price/preprocessed\"\n",
    "TEMP_FOLDER_PRICE = f\"stock_price/preprocessed/price_{STOCK_SYMBOL_LOWER}\"\n",
    "\n",
    "# Write to a single CSV file\n",
    "df_price.coalesce(1).write.csv(\n",
    "    TEMP_FOLDER_PRICE, sep=\",\", header=True, mode=\"overwrite\"\n",
    ")\n",
    "\n",
    "# Move the part file to the desired filename\n",
    "for filename in os.listdir(TEMP_FOLDER_PRICE):\n",
    "    if filename.startswith(\"part-\"):\n",
    "        shutil.move(\n",
    "            os.path.join(TEMP_FOLDER_PRICE, filename),\n",
    "            os.path.join(FOLDER_PRICE, FILE_NAME_PRICE),\n",
    "        )\n",
    "\n",
    "# Remove the temporary directory\n",
    "shutil.rmtree(TEMP_FOLDER_PRICE)\n",
    "\n",
    "# Uncache the dataframe in a non-blocking operation to free up memory.\n",
    "df_price.unpersist(blocking=False)\n",
    "\n",
    "local_variables = [\n",
    "    \"df_price\",\n",
    "]\n",
    "\n",
    "# Delete local variables if they exist\n",
    "for var in local_variables:\n",
    "    if var in locals():  # Check if the variable exists in the local scope\n",
    "        del locals()[var]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the News Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the external news and Nasdaq news datasets from CSV files to Spark dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read news data from CSV files to Spark dataframes\n",
    "df_external_news = spark.read.csv(\n",
    "    FILE_PATH_EXTERNAL_NEWS, header=True, schema=EXTERNAL_NEWS_SCHEMA\n",
    ")\n",
    "df_nasdaq_news = spark.read.csv(\n",
    "    FILE_PATH_NASDAQ_NEWS, header=True, schema=NASDAQ_NEWS_SCHEMA\n",
    ")\n",
    "\n",
    "# Store in memory\n",
    "df_external_news.persist()\n",
    "df_nasdaq_news.persist()\n",
    "\n",
    "# Verify the dataframes\n",
    "print(f\"Row count for df_external_news: {df_external_news.count()}\")\n",
    "df_external_news.show(5, truncate=True)\n",
    "\n",
    "print(f\"Row count for df_nasdaq_news: {df_nasdaq_news.count()}\")\n",
    "df_nasdaq_news.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop unused fields and merge the external and Nasdaq news datasets into one dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unused fields\n",
    "df_external_news = df_external_news.drop(\n",
    "    \"Luhn_summary\", \"Textrank_summary\", \"Lexrank_summary\"\n",
    ")\n",
    "df_nasdaq_news = df_nasdaq_news.drop(\n",
    "    \"Unnamed: 0\", \"Luhn_summary\", \"Textrank_summary\", \"Lexrank_summary\"\n",
    ")\n",
    "\n",
    "# Merge two dataframes\n",
    "df_news = df_nasdaq_news.unionByName(df_external_news)\n",
    "\n",
    "# Store in memory\n",
    "df_news.persist()\n",
    "\n",
    "# Verify\n",
    "# Expect count is 7419, where 2945 + 4474 = 7419\n",
    "print(f\"Row count for df_news: {df_news.count()}\")\n",
    "df_news.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the merged news dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the timestamps to UTC timezone. Example: Convert \"2019-01-15 00:00:00 UTC\" to \"2019-01-15 00:00:00\".\n",
    "df_news = df_news.withColumn(\n",
    "    \"Date\", F.to_utc_timestamp(F.to_timestamp(\"Date\", \"yyyy-MM-dd HH:mm:ss zzz\"), \"UTC\")\n",
    ").filter(F.col(\"Date\").isNotNull())\n",
    "\n",
    "# Add a \"Summarized\" field with all values set to 0.\n",
    "# Add a \"Sentiment_score\" field with all values set to 0.\n",
    "# Sort by Date field in descending order.\n",
    "df_news = (\n",
    "    df_news.withColumn(\"Summarized\", F.lit(0))\n",
    "    .withColumn(\"Sentiment_score\", F.lit(0))\n",
    "    .orderBy(\"Date\", ascending=False)\n",
    ")\n",
    "\n",
    "# Add a unique identifier field\n",
    "uuid_udf = F.udf(lambda: str(uuid.uuid4()), StringType())\n",
    "df_news = df_news.withColumn(\"UUID\", uuid_udf())\n",
    "\n",
    "# Verify\n",
    "print(f\"Row count for df_news: {df_news.count()}\")\n",
    "df_news.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save a copy of the preprocessed merged news dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify directories\n",
    "FOLDER_MERGED_NEWS = \"stock_news/merged\"\n",
    "TEMP_FOLDER_MERGED_NEWS = f\"stock_news/merged/news_{STOCK_SYMBOL_LOWER}\"\n",
    "\n",
    "# Write to a single CSV file\n",
    "df_news.coalesce(1).write.csv(\n",
    "    TEMP_FOLDER_MERGED_NEWS, sep=\",\", header=True, mode=\"overwrite\"\n",
    ")\n",
    "\n",
    "# Move the part file to the desired filename\n",
    "for filename in os.listdir(TEMP_FOLDER_MERGED_NEWS):\n",
    "    if filename.startswith(\"part-\"):\n",
    "        shutil.move(\n",
    "            os.path.join(TEMP_FOLDER_MERGED_NEWS, filename),\n",
    "            os.path.join(FOLDER_MERGED_NEWS, FILE_NAME_NEWS),\n",
    "        )\n",
    "\n",
    "# Remove the temporary directory\n",
    "shutil.rmtree(TEMP_FOLDER_MERGED_NEWS)\n",
    "\n",
    "# Uncache the dataframe in a non-blocking operation to free up memory.\n",
    "df_external_news.unpersist(blocking=False)\n",
    "df_nasdaq_news.unpersist(blocking=False)\n",
    "df_news.unpersist(blocking=False)\n",
    "\n",
    "local_variables = [\n",
    "    \"df_external_news\",\n",
    "    \"df_nasdaq_news\",\n",
    "    \"df_news\",\n",
    "    \"uuid_udf\",\n",
    "]\n",
    "\n",
    "# Delete local variables if they exist\n",
    "for var in local_variables:\n",
    "    if var in locals():  # Check if the variable exists in the local scope\n",
    "        del locals()[var]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize the News Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the merged news dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file to Spark dataframe\n",
    "df_merged_news = spark.read.csv(\n",
    "    FILE_PATH_MERGED_NEWS, header=True, schema=MERGED_NEWS_SCHEMA\n",
    ")\n",
    "\n",
    "# Store in memory\n",
    "df_merged_news.persist()\n",
    "\n",
    "# Verify\n",
    "print(f\"Row count for df_merged_news: {df_merged_news.count()}\")\n",
    "df_merged_news.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the news texts using a LSA Summarizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Description:\n",
    "Takes the article text as input, parses it using PlaintextParser, and summarizes it using LsaSummarizer.\n",
    "\n",
    "Parameters:\n",
    "text (string): The news article.\n",
    "\n",
    "Returns:\n",
    "string: The summarized text.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def summarize_article(text):\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(LANGUAGE))\n",
    "\n",
    "    # Stemming reduces words to their root form for the summarizer to identify similar concepts expressed with\n",
    "    # different word forms.\n",
    "    stemmer = Stemmer(LANGUAGE)\n",
    "\n",
    "    # Initializes the summarizer with the stemmer\n",
    "    summarizer = LsaSummarizer(stemmer)\n",
    "\n",
    "    # Removes stop word to eliminate common non-keyword words.\n",
    "    summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "\n",
    "    # Generates the summarized text\n",
    "    summary = summarizer(parser.document, SENTENCES_COUNT)\n",
    "    return \" \".join([str(sentence) for sentence in summary])\n",
    "\n",
    "\n",
    "# A user-designed function to wrap the summarize_article function to be used in Spark.\n",
    "summarize_udf = F.udf(summarize_article, StringType())\n",
    "\n",
    "# Concats both fields with a period and space as the separator.\n",
    "# Execute the summarize_udf function\n",
    "df_merged_news = df_merged_news.withColumn(\n",
    "    \"Text\", F.concat_ws(\". \", \"Article_title\", \"Article\")\n",
    ").withColumn(\"Lsa_summary\", summarize_udf(\"Text\"))\n",
    "\n",
    "# Updated summarized indicator\n",
    "df_merged_news = df_merged_news.withColumn(\n",
    "    \"Summarized\",\n",
    "    F.when(\n",
    "        F.col(\"Lsa_summary\").isNotNull() & (F.col(\"Lsa_summary\") != \"\"), F.lit(1)\n",
    "    ).otherwise(0),\n",
    ")\n",
    "\n",
    "# Verify number of news with no summary\n",
    "# print(\n",
    "#     f\"Number of news with no summary: {df_merged_news.filter(col(\"Summarized\") == 0).count()}\"\n",
    "# )\n",
    "\n",
    "# Remove rows without a summary\n",
    "df_merged_news = df_merged_news.filter((F.col(\"Summarized\") == 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the results of the summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify\n",
    "print(f\"Row count for df_merged_news: {df_merged_news.count()}\")\n",
    "\n",
    "df_merged_news.show(5, truncate=True)\n",
    "\n",
    "df_merged_news.select(\n",
    "    \"Date\",\n",
    "    \"Summarized\",\n",
    "    \"Text\",\n",
    "    \"Lsa_summary\",\n",
    ").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save a copy of the summarized news dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directories\n",
    "FOLDER_SUMMARIZED_NEWS = \"stock_news/summarized\"\n",
    "TEMP_FOLDER_SUMMARIZED_NEWS = \"stock_news/summarized/news_{STOCK_SYMBOL_LOWER}\"\n",
    "\n",
    "# Write to a single CSV file\n",
    "df_merged_news.coalesce(1).write.csv(\n",
    "    TEMP_FOLDER_SUMMARIZED_NEWS, sep=\",\", header=True, mode=\"overwrite\"\n",
    ")\n",
    "\n",
    "# Move the part file to the desired filename\n",
    "for filename in os.listdir(TEMP_FOLDER_SUMMARIZED_NEWS):\n",
    "    if filename.startswith(\"part-\"):\n",
    "        shutil.move(\n",
    "            os.path.join(TEMP_FOLDER_SUMMARIZED_NEWS, filename),\n",
    "            os.path.join(FOLDER_SUMMARIZED_NEWS, FILE_NAME_NEWS),\n",
    "        )\n",
    "\n",
    "# Remove the temporary directory\n",
    "shutil.rmtree(TEMP_FOLDER_SUMMARIZED_NEWS)\n",
    "\n",
    "# Uncache the dataframe in a non-blocking operation to free up memory.\n",
    "df_merged_news.unpersist(blocking=False)\n",
    "\n",
    "local_variables = [\n",
    "    \"df_merged_news\",\n",
    "    \"summarize_udf\",\n",
    "]\n",
    "\n",
    "# Delete local variables if they exist\n",
    "for var in local_variables:\n",
    "    if var in locals():  # Check if the variable exists in the local scope\n",
    "        del locals()[var]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the Sentiment Score of the News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the summarized news dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file to Spark dataframe\n",
    "df_summarized_news = spark.read.csv(\n",
    "    FILE_PATH_SUMMARIZED_NEWS, header=True, schema=SUMMARIZED_NEWS_SCHEMA\n",
    ")\n",
    "\n",
    "# Store in memory\n",
    "df_summarized_news.persist()\n",
    "\n",
    "# Verify\n",
    "print(f\"Row count for df_summarized_news: {df_summarized_news.count()}\")\n",
    "df_summarized_news.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the summaries and initialize the large-language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the UUID, Date, Url, and Lsa_summary from the first row\n",
    "collection = df_summarized_news.select(\n",
    "    F.collect_list(\"UUID\"),\n",
    "    F.collect_list(\"Date\"),\n",
    "    F.collect_list(\"Lsa_summary\"),\n",
    ").first()\n",
    "uuids = collection[0]\n",
    "dates = collection[1]\n",
    "lsa_summaries = collection[2]\n",
    "\n",
    "# Verify\n",
    "print(f\"Number of uuids: {len(uuids)}\")\n",
    "print(f\"Number of dates: {len(dates)}\")\n",
    "print(f\"Number of lsa_summaries: {len(lsa_summaries)}\")\n",
    "\n",
    "# Initializing the OllamaLLM\n",
    "try:\n",
    "    llm = ChatOllama(\n",
    "        model=MODEL, temperature=TEMPERATURE, num_predict=MAX_OUTPUT_TOKENS\n",
    "    )\n",
    "    print(\"ChatOllama instance created successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"Error creating ChatOllama instance:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed the summmmaries to the model in batches and capture the resulting sentiment score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holds the resulting output, i.e. the sentiment scores\n",
    "sentiments = []\n",
    "\n",
    "# Iterate in batches\n",
    "for i in range(0, len(lsa_summaries), BATCH_SIZE):\n",
    "    batch_summaries = lsa_summaries[i : i + BATCH_SIZE]\n",
    "    batch_uuids = uuids[i : i + BATCH_SIZE]\n",
    "\n",
    "    num_text = len(batch_summaries)\n",
    "    # print(f\"#{i}, num_text: {num_text}\")\n",
    "\n",
    "    batch_text = \" \".join(\n",
    "        [\n",
    "            f\"### {STOCK_SYMBOL_UPPER} Stock News: {summary} \"\n",
    "            for summary in batch_summaries\n",
    "        ]\n",
    "    )\n",
    "    # print(f\"#{i}, batch_text: {batch_text}\")\n",
    "\n",
    "    chat_template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                f\"Forget all previous instructions. You are a financial expert with stock recommendation experience. Based on news for a specific stock, provide a sentiment score in the range of 1 to 5 inclusive, where 1 is negative, 2 is somewhat negative, 3 is neutral, 4 is somewhat positive, and 5 is positive. {num_text} summerized news will be passed in each time. You will provide {num_text} scores, one score for each of the summerized news in the format as shown below in the response from the assistant.\",\n",
    "            ),\n",
    "            (\n",
    "                \"user\",\n",
    "                f\"### AAPL Stock News: Apple (AAPL) increased 22%. ### AAPL Stock News: Apple (AAPL) price decreased 30%. ### MSFT Stock News: Microsoft (MSTF) price has not changed. ### AAPL Stock News: Apple (AAPL) announced the new iPhone 15. ### AAPL Stock News: Apple (AAPL) will release the Vison Pro on Feb 2, 2024.\",\n",
    "            ),\n",
    "            (\"ai\", \"5, 1, 3, 4, 4\"),\n",
    "            (\"user\", batch_text),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    messages = chat_template.format_messages(num_text=num_text)\n",
    "    # print(f\"#{i}, messages: {messages}\")\n",
    "\n",
    "    response = llm.invoke(messages)\n",
    "    print(f\"#{i}: {response.content}\")\n",
    "\n",
    "    # Loop through each batch and append the sentiment scores to the list\n",
    "    for sentiment in response.content.split(\",\"):\n",
    "        stripped_sentiment = sentiment.strip()\n",
    "        try:\n",
    "            if stripped_sentiment:  # Check if the stripped sentiment is not empty\n",
    "                # Round the sentiment and clamp it between 1 and 5\n",
    "                rounded_sentiment = max(\n",
    "                    MIN_VALUE, min(MAX_VALUE, round(float(stripped_sentiment)))\n",
    "                )\n",
    "                sentiments.append(rounded_sentiment)\n",
    "        except ValueError:\n",
    "            print(f\"Invalid sentiment value: {stripped_sentiment}\")\n",
    "        # print(f\"#{i}, sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the sentiments to a file\n",
    "with open(FILE_PATH_SENTIMENT_SCORES, \"w\") as f:\n",
    "    f.writelines(f\"{sentiment}\\n\" for sentiment in sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify\n",
    "print(f\"Number of lsa_summaries: {len(lsa_summaries)}\")\n",
    "print(f\"Number of sentiments: {len(sentiments)}\")\n",
    "print(f\"Row count for df_summarized_news: {df_summarized_news.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO BE EXECUTED ONLY IF THE NUMBER OF SENTIMENTS DOES NOT MATCH THE NUMBER OF SUMMARIES\n",
    "# Step 1: Re-run the sentiment score for the affected batch\n",
    "# Step 2: Update the missing scores to the sentiments.csv file\n",
    "# Step 3: Verify the number of sentiments in the sentiments.csv file matches the number of summaries\n",
    "# Step 4: Execute code below to copy out the sentiments from sentiments.csv file to the sentiments list\n",
    "\n",
    "sentiment = []\n",
    "with open(FILE_PATH_SENTIMENT_SCORES, \"r\") as f:\n",
    "    sentiments = [int(sentiment.strip()) for sentiment in f if sentiment.strip()]\n",
    "print(f\"Number of sentiments: {len(sentiments)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save a copy of the sentiment scored news dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(sentiments) != len(lsa_summaries):\n",
    "    raise ValueError(\"The number of sentiments does not match the number of summaries.\")\n",
    "\n",
    "# Combine lists into a list of tuples\n",
    "scored_news = list(zip(uuids, dates, sentiments, lsa_summaries))\n",
    "\n",
    "# Create a dataframe with the specified schema\n",
    "df_scored_news = spark.createDataFrame(scored_news, schema=SCORED_NEWS_SCHEMA)\n",
    "\n",
    "# Specify the directories\n",
    "FOLDER_SCORED_NEWS = \"stock_news/scored\"\n",
    "TEMP_FOLDER_SCORED_NEWS = \"stock_news/scored/news_{STOCK_SYMBOL_LOWER}\"\n",
    "\n",
    "# Write to a single CSV file\n",
    "df_scored_news.coalesce(1).write.csv(\n",
    "    TEMP_FOLDER_SCORED_NEWS, sep=\",\", header=True, mode=\"overwrite\"\n",
    ")\n",
    "\n",
    "# Move the part file to the desired filename\n",
    "for filename in os.listdir(TEMP_FOLDER_SCORED_NEWS):\n",
    "    if filename.startswith(\"part-\"):\n",
    "        shutil.move(\n",
    "            os.path.join(TEMP_FOLDER_SCORED_NEWS, filename),\n",
    "            os.path.join(FOLDER_SCORED_NEWS, FILE_NAME_NEWS),\n",
    "        )\n",
    "\n",
    "# Remove the temporary directory\n",
    "shutil.rmtree(TEMP_FOLDER_SCORED_NEWS)\n",
    "\n",
    "# Uncache the dataframe in a non-blocking operation to free up memory.\n",
    "df_summarized_news.unpersist(blocking=False)\n",
    "\n",
    "local_variables = [\n",
    "    \"collection\",\n",
    "    \"uuids\",\n",
    "    \"dates\",\n",
    "    \"lsa_summaries\",\n",
    "    \"sentiments\",\n",
    "    \"llm\",\n",
    "    \"chat_template\",\n",
    "    \"messages\",\n",
    "    \"response\",\n",
    "    \"scored_news\",\n",
    "    \"df_summarized_news\",\n",
    "    \"df_scored_news\",\n",
    "]\n",
    "\n",
    "# Delete local variables if they exist\n",
    "for var in local_variables:\n",
    "    if var in locals():  # Check if the variable exists in the local scope\n",
    "        del locals()[var]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exponentially Decay the Sentiment Score of the News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the sentiment scored stock news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scored_news = spark.read.csv(\n",
    "    FILE_PATH_SCORED_NEWS, header=True, schema=SCORED_NEWS_SCHEMA\n",
    ")\n",
    "\n",
    "# Verify\n",
    "print(f\"Row count for df_scored_news: {df_scored_news.count()}\")\n",
    "df_scored_news.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the average sentiment scores for each date. Then, populate the missing dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting only the date component and sort by date\n",
    "df_scored_news_copy = df_scored_news.select(\n",
    "    \"UUID\", F.to_date(\"Date\").alias(\"Date\"), \"Sentiment_score\", \"Lsa_summary\"\n",
    ").orderBy(\"Date\")\n",
    "\n",
    "# Group by \"Date\" and calculate the average \"Sentiment_score\"\n",
    "df_avg_sentiment = df_scored_news_copy.groupBy(\"Date\").agg(\n",
    "    F.avg(\"Sentiment_score\").alias(\"Sentiment_avg\")\n",
    ")\n",
    "\n",
    "# Retrieve the start and end date\n",
    "start_date = df_avg_sentiment.agg(F.min(\"Date\")).collect()[0][0]\n",
    "end_date = df_avg_sentiment.agg(F.max(\"Date\")).collect()[0][0]\n",
    "\n",
    "# Initialize a dataframe with the start and end dates\n",
    "df_date_range = spark.createDataFrame(\n",
    "    [(start_date, end_date)], [\"start_date\", \"end_date\"]\n",
    ")\n",
    "\n",
    "# Generate a date range using sequence\n",
    "df_dates = df_date_range.select(\n",
    "    F.expr(\"sequence(to_date(start_date), to_date(end_date), interval 1 day)\").alias(\n",
    "        \"Date\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Explode the array to get individual dates\n",
    "df_dates = df_dates.select(F.explode(\"Date\").alias(\"Date\"))\n",
    "\n",
    "# Join the complete date range with the original dataframe\n",
    "df_avg_sentiment_filled = df_dates.join(df_avg_sentiment, on=\"Date\", how=\"left\")\n",
    "\n",
    "# Store in memory\n",
    "df_avg_sentiment_filled.persist()\n",
    "\n",
    "# Verify\n",
    "print(f\"Row count for df_avg_sentiment_filled: {df_avg_sentiment_filled.count()}\")\n",
    "df_avg_sentiment_filled.show(df_avg_sentiment_filled.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply an exponential decay algorithm to the missing average sentiment scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a window specification to get the last valid sentiment and last valid date\n",
    "window_spec = Window.orderBy(\"Date\").rowsBetween(\n",
    "    Window.unboundedPreceding, Window.currentRow\n",
    ")\n",
    "\n",
    "# Get last valid sentiment\n",
    "df_avg_sentiment_filled = df_avg_sentiment_filled.withColumn(\n",
    "    \"Last_valid_sentiment\", F.last(\"Sentiment_avg\", ignorenulls=True).over(window_spec)\n",
    ")\n",
    "\n",
    "# Get last valid date only when sentiment is not null\n",
    "df_avg_sentiment_filled = df_avg_sentiment_filled.withColumn(\n",
    "    \"Last_valid_date\",\n",
    "    F.last(\n",
    "        F.when(F.col(\"Sentiment_avg\").isNotNull(), F.col(\"Date\")), ignorenulls=True\n",
    "    ).over(window_spec),\n",
    ")\n",
    "\n",
    "# Calculate the number of days since the last valid sentiment\n",
    "df_avg_sentiment_filled = df_avg_sentiment_filled.withColumn(\n",
    "    \"Days_since_last_valid\", (F.datediff(F.col(\"Date\"), F.col(\"Last_valid_date\")))\n",
    ")\n",
    "\n",
    "# Calculate decayed sentiment for rows where the average sentiment is null\n",
    "df_avg_sentiment_filled = df_avg_sentiment_filled.withColumn(\n",
    "    \"Decayed_sentiment\",\n",
    "    F.when(\n",
    "        F.col(\"Sentiment_avg\").isNull(),\n",
    "        BASE_VALUE\n",
    "        + (F.col(\"Last_valid_sentiment\") - BASE_VALUE)\n",
    "        * F.exp(-DECAY_RATE * F.col(\"Days_since_last_valid\")),\n",
    "    ).otherwise(F.col(\"Sentiment_avg\")),\n",
    ")\n",
    "\n",
    "# Verify\n",
    "print(f\"Row count for df_avg_sentiment_filled: {df_avg_sentiment_filled.count()}\")\n",
    "df_avg_sentiment_filled.show(df_avg_sentiment_filled.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save a copy of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directories\n",
    "FOLDER_DECAYED_NEWS = \"stock_news/decayed\"\n",
    "TEMP_FOLDER_DECAYED_NEWS = \"stock_news/decayed/news_{STOCK_SYMBOL_LOWER}\"\n",
    "\n",
    "# Write to a single CSV file\n",
    "df_avg_sentiment_filled.coalesce(1).write.csv(\n",
    "    TEMP_FOLDER_DECAYED_NEWS, sep=\",\", header=True, mode=\"overwrite\"\n",
    ")\n",
    "\n",
    "# Move the part file to the desired filename\n",
    "for filename in os.listdir(TEMP_FOLDER_DECAYED_NEWS):\n",
    "    if filename.startswith(\"part-\"):\n",
    "        shutil.move(\n",
    "            os.path.join(TEMP_FOLDER_DECAYED_NEWS, filename),\n",
    "            os.path.join(FOLDER_DECAYED_NEWS, FILE_NAME_NEWS),\n",
    "        )\n",
    "\n",
    "# Remove the temporary directory\n",
    "shutil.rmtree(TEMP_FOLDER_DECAYED_NEWS)\n",
    "\n",
    "# Uncache the dataframe in a non-blocking operation to free up memory.\n",
    "df_avg_sentiment_filled.unpersist(blocking=False)\n",
    "\n",
    "local_variables = [\n",
    "    \"window_spec\",\n",
    "    \"start_date\",\n",
    "    \"end_date\",\n",
    "    \"df_scored_news\",\n",
    "    \"df_scored_news_copy\",\n",
    "    \"df_avg_sentiment\",\n",
    "    \"df_date_range\",\n",
    "    \"df_dates\",\n",
    "    \"df_avg_sentiment_filled\",\n",
    "]\n",
    "\n",
    "# Delete local variables if they exist\n",
    "for var in local_variables:\n",
    "    if var in locals():  # Check if the variable exists in the local scope\n",
    "        del locals()[var]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrate the News and Price Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ingest the decayed sentiment dataset and the price dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read price data from CSV files to Spark dataframes\n",
    "df_decayed_sentiment = spark.read.csv(\n",
    "    FILE_PATH_DECAYED_NEWS, header=True, schema=DECAYED_SENTIMENT_SCHEMA\n",
    ")\n",
    "df_price = spark.read.csv(\n",
    "    FILE_PATH_PREPROCESSED_PRICE, header=True, schema=PREPROCESSED_PRICE_SCHEMA\n",
    ")\n",
    "\n",
    "# Verify\n",
    "print(f\"Row count for df_decayed_sentiment: {df_decayed_sentiment.count()}\")\n",
    "df_decayed_sentiment.show(5, truncate=True)\n",
    "print(f\"Row count for df_price: {df_price.count()}\")\n",
    "df_price.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the price dataset with the average sentiment score taken from news dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unused fields\n",
    "df_decayed_sentiment = df_decayed_sentiment.drop(\n",
    "    \"Sentiment_avg\", \"Last_valid_sentiment\", \"Last_valid_date\", \"Days_since_last_valid\"\n",
    ")\n",
    "\n",
    "# Join the price data with the average sentiment data\n",
    "df_combined = df_price.join(df_decayed_sentiment, on=\"Date\", how=\"left\")\n",
    "\n",
    "# Store in memory\n",
    "df_combined.persist()\n",
    "\n",
    "# Convert the decayed sentiment values to 3 if they are null\n",
    "df_combined = df_combined.withColumn(\n",
    "    \"Decayed_sentiment\",\n",
    "    F.when(\n",
    "        F.col(\"Decayed_sentiment\").isNull() | (F.col(\"Decayed_sentiment\") == \"\"),\n",
    "        BASE_VALUE,\n",
    "    ).otherwise(F.col(\"Decayed_sentiment\")),\n",
    ")\n",
    "\n",
    "# Verify\n",
    "df_combined.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the sentiments to between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the Decayed_sentiment column\n",
    "df_combined = df_combined.withColumn(\n",
    "    \"Normalized_sentiment\",\n",
    "    (F.col(\"Decayed_sentiment\") - MIN_VALUE) / (MAX_VALUE - MIN_VALUE),\n",
    ")\n",
    "\n",
    "# Verify\n",
    "df_combined.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save a copy of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directories\n",
    "FOLDER_COMBINED = \"stock_combined\"\n",
    "TEMP_FOLDER_COMBINED = \"stock_combined/{STOCK_SYMBOL_LOWER}\"\n",
    "\n",
    "# Write to a single CSV file\n",
    "df_combined.coalesce(1).write.csv(\n",
    "    TEMP_FOLDER_COMBINED, sep=\",\", header=True, mode=\"overwrite\"\n",
    ")\n",
    "\n",
    "# Move the part file to the desired filename\n",
    "for filename in os.listdir(TEMP_FOLDER_COMBINED):\n",
    "    if filename.startswith(\"part-\"):\n",
    "        shutil.move(\n",
    "            os.path.join(TEMP_FOLDER_COMBINED, filename),\n",
    "            os.path.join(FOLDER_COMBINED, FILE_NAME_COMBINED),\n",
    "        )\n",
    "\n",
    "# Remove the temporary directory\n",
    "shutil.rmtree(TEMP_FOLDER_COMBINED)\n",
    "\n",
    "# Uncache the dataframe in a non-blocking operation to free up memory.\n",
    "df_combined.unpersist(blocking=False)\n",
    "\n",
    "local_variables = [\n",
    "    \"df_decayed_sentiment\",\n",
    "    \"df_price\",\n",
    "    \"df_combined\",\n",
    "]\n",
    "\n",
    "# Delete local variables if they exist\n",
    "for var in local_variables:\n",
    "    if var in locals():  # Check if the variable exists in the local scope\n",
    "        del locals()[var]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
